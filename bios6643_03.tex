% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  9pt,
  ignorenonframetext,
]{beamer}
\usepackage{pgfpages}
% set the section number 
\setbeamertemplate{section in toc}[sections numbered]
\setbeamertemplate{subsection in toc}[subsections numbered]
\setbeamertemplate{navigation symbols}{}
% set the page
\setbeamertemplate{footline}[page number]
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty

%%
%%% Definition of colors
%%% Source: https://latexcolor.com/
\definecolor{blanchedalmond}{rgb}{1.0, 0.92, 0.8}
\definecolor{blond}{rgb}{0.98, 0.94, 0.75}
%%% End of definition of colors
%%

% Prevent slide breaks in the middle of a paragraph
\widowpenalties 1 10000
\raggedbottom
\setbeamertemplate{part page}{
  \centering
  \begin{beamercolorbox}[sep=16pt,center]{part title}
    \usebeamerfont{part title}\insertpart\par
  \end{beamercolorbox}
}
\setbeamertemplate{section page}{
  \centering
  \begin{beamercolorbox}[sep=12pt,center]{part title}
    \usebeamerfont{section title}\insertsection\par
  \end{beamercolorbox}
}
\setbeamertemplate{subsection page}{
  \centering
  \begin{beamercolorbox}[sep=8pt,center]{part title}
    \usebeamerfont{subsection title}\insertsubsection\par
  \end{beamercolorbox}
}
\AtBeginPart{
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \frame{\subsectionpage}
}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usetheme[]{Goettingen}
\usecolortheme{rose}
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
  
% adjust for sidebar
  \setbeamertemplate{sidebar \beamer@sidebarside}
  {
    \beamer@tempdim=\beamer@sidebarwidth%
    \advance\beamer@tempdim by -6pt%
    \vskip4em%
    \insertverticalnavigation{\beamer@sidebarwidth}%
    \vfill
    \ifx\beamer@sidebarside\beamer@lefttext%
    \else%
      \usebeamercolor{normal text}%
      \llap{\usebeamertemplate***{navigation symbols}\hskip0.1cm}%
      \vskip2pt%
    \fi%
  }%

  \ifx\beamer@sidebarside\beamer@lefttext%
    \defbeamertemplate*{sidebar right}{sidebar theme}
    {%
      \vfill%
      \llap{\usebeamertemplate***{navigation symbols}\hskip0.1cm}%
      \vskip2pt}
  \fi

\setbeamertemplate{section in sidebar}%{sidebar theme}
{%
  \vbox{%
    \vskip1ex%
    \beamer@sidebarformat{3pt}{section in sidebar}{\insertsectionheadnumber
~\insertsectionhead}%
  }%
}
\setbeamertemplate{section in sidebar shaded}%{sidebar theme}
{%
  \vbox{%
    \vskip1ex%
    \beamer@sidebarformat{3pt}{section in sidebar shaded}{\insertsectionheadnumber
~\insertsectionhead}%
  }%
}  

\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={BIOS6643 longitudinal},
  pdfauthor={EJC},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\newif\ifbibliography
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering

%
% When using babel or polyglossia with biblatex, loading csquotes is recommended 
% to ensure that quoted texts are typeset according to the rules of your main language.
%
\usepackage{csquotes}

%
% blockquote
%
\definecolor{blockquote-border}{RGB}{221,221,221}
\definecolor{blockquote-text}{RGB}{89,89,89}
\usepackage{mdframed}
\newmdenv[rightline=false,bottomline=false,topline=false,linewidth=3pt,linecolor=blockquote-border,skipabove=\parskip]{customblockquote}
\renewenvironment{quote}{\begin{customblockquote}\list{}{\rightmargin=0em\leftmargin=0em}%
\item\relax\color{blockquote-text}\ignorespaces}{\unskip\unskip\endlist\end{customblockquote}}

%
% Source Sans Pro as the de­fault font fam­ily
% Source Code Pro for monospace text
%
% 'default' option sets the default 
% font family to Source Sans Pro, not \sfdefault.
%
\usepackage[default]{sourcesanspro}
\usepackage{sourcecodepro}

% XeLaTeX specific adjustments for straight quotes: https://tex.stackexchange.com/a/354887
% This issue is already fixed (see https://github.com/silkeh/latex-sourcecodepro/pull/5) but the 
% fix is still unreleased.
% TODO: Remove this workaround when the new version of sourcecodepro is reelased on CTAN.
\ifxetex
\makeatletter
\defaultfontfeatures[\ttfamily]
  { Numbers   = \sourcecodepro@figurestyle,
    Scale     = \SourceCodePro@scale,
    Extension = .otf }
\setmonofont
  [ UprightFont    = *-\sourcecodepro@regstyle,
    ItalicFont     = *-\sourcecodepro@regstyle It,
    BoldFont       = *-\sourcecodepro@boldstyle,
    BoldItalicFont = *-\sourcecodepro@boldstyle It ]
  {SourceCodePro}
\makeatother
\fi

\AtBeginSubsection{}
\AtBeginSection{}

\title{BIOS6643 longitudinal}
\subtitle{L3 Linear Mixed Model}
\author{EJC}
\date{}
\institute{Department of Biostatistics \& Informatics}

\begin{document}
\frame{\titlepage}

\begin{frame}
  \begin{columns}
  \column{10cm}
  \tableofcontents
  \end{columns}
\end{frame}
\hypertarget{linear-mixed-model}{%
\section{Linear Mixed Model}\label{linear-mixed-model}}

\begin{frame}{Topics for these notes:}
\protect\hypertarget{topics-for-these-notes}{}
\begin{itemize}
\tightlist
\item
  Notation, model assumptions and comments
\item
  LMM's versus GLM's
\item
  Some simpler LMMs discussed in 6612
\item
  Important distributions in the LMM
\item
  General parameter estimation in the LMM; ML and REML
\end{itemize}

\textbf{Associated reading: The LMM course notes (early chapters)}
\end{frame}

\begin{frame}{Notation, model assumptions and comments}
\protect\hypertarget{notation-model-assumptions-and-comments}{}
\begin{itemize}
\item
  considering longitudinal data collected on subjects, there are 3 basic
  ways that linear mixed models can be expressed:
\item
  subject-time level
\item
  subject level
\item
  complete data level
\item
  The mixed model at the subject-time level is useful when you have
  defined the particular experiment and variables. For example, most of
  the models written in the notes up to this point are explicitly
  defined mixed models expressed at the subject-time level, with
  response \(Y_{ij}\) (\(i\) denoting subject, \(j\) denoting time).
\end{itemize}
\end{frame}

\begin{frame}{}
\protect\hypertarget{section}{}
\begin{block}{We can write a more general mixed model in terms of
subject data:}
\protect\hypertarget{we-can-write-a-more-general-mixed-model-in-terms-of-subject-data}{}
\(Y_i = X_i \beta + Z_ib_i + \epsilon_i\) for subjects
\(i = 1,\ ...\ ,\ n\).

\(Y_i\) are the \(r_i \times 1\) responses for subject \(i\)

\(X_i\) is the matrix of known covariates associated with fixed effects

\(\beta\) are the \(p \times 1\) fixed effects

\(Z_i\) is the matrix of known covariates associated with the random
effects

\(\epsilon_i\) is the residual error vector.

\begin{itemize}
\tightlist
\item
  We index \(X\) and \(Z\) by subject even when they may be the same
  across subjects, in order to identify the size of the matrices. - We
  will keep \(X\) and \(Z\) without indices to denote the full-data
  versions of these matrices, which will be defined shortly.
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{}
\protect\hypertarget{section-1}{}
\begin{itemize}
\item
  For the model above, we assume
  \(b_i \sim \mathcal N(\pmb0,\ \pmb G_i)\) and
  \(\epsilon_i \sim \mathcal N(\pmb 0, \pmb R_i)\), and that these
  random vectors are independent.
\item
  In addition, subjects themselves are assumed to be independent of each
  other. However, in cases where subjects are not independent, we can
  work this into the model by defining appropriate cluster units, which
  will be discussed later.
\item
  Generally speaking, \(\pmb G_i\) will be used to account for
  variability between subjects and \(\pmb R_i\) will be used to account
  for covariances between repeated measures within subjects. However, it
  will also be demonstrated that there are many ways to model correlated
  data that combine \(\pmb G_i\) and \(R_i\).
\end{itemize}
\end{frame}

\begin{frame}{}
\protect\hypertarget{section-2}{}
\begin{itemize}
\tightlist
\item
  The subject models can be combined into one `complete-data' model by
  essentially stacking the \(n\) subject-specific models:
\end{itemize}

\[
\begin{bmatrix}
Y_{1 (r_1 \times 1)}\\Y_{2 (r_2 \times 1)}\\ \vdots\\ Y_{n (r_n \times 1)}
\end{bmatrix} =
\begin{bmatrix}
X_{1 (r_1 \times p)}\\X_{2(r_2 \times p)}\\ \vdots\\ X_{n(r_n \times p)}
\end{bmatrix}
\times \pmb \beta_{(p \times 1)} +
\begin{bmatrix}
Z_{1 (r_1 \times q)}\ 0\ \dots\ 0\\ 0\ Z_{2 (r_2 \times q)}\ \dots\ 0\\ \vdots\ \ddots\ \ddots\ \vdots\\ 0\ 0\ \dots\ Z_{n (r_n \times q)}
\end{bmatrix}
\begin{bmatrix}
b_{1 (q \times 1)}\\b_{2 (q \times 1)}\\ \vdots\\ b_{n (q \times 1)}
\end{bmatrix} +
\begin{bmatrix}
\epsilon_{1 (r_1 \times 1)}\\ \epsilon_{2 (r_2 \times 1)}\\ \vdots\\ \epsilon_{n (r_n \times 1)}
\end{bmatrix}
\]

or more succinctly
\(Y_{r_{tot} \times 1} = X_{r_{tot} \times p} \beta_{p \times 1} + Z_{r_{tot}\times q_{tot}} \times b_{q_{tot} \times 1} + \epsilon_{r_{tot} \times 1}\),\\
where

\[
\begin{pmatrix} \pmb b_{(q_{tot} \times 1)} \\ \pmb \epsilon_{(r_{tot} \times 1)}
\end{pmatrix}
\sim \mathcal N 
\begin{pmatrix}
\begin{pmatrix}
\pmb 0_{(q_{tot} \times 1)}\\ \pmb 0_{(r_{tot} \times 1)}
\end{pmatrix} ,
\begin{pmatrix}
\pmb G_{(q_{tot} \times q_{tot}})\ \ \ \pmb 0_{(q_{tot} \times r_{tot})}\\ \pmb 0_{(r_{tot} \times q_{tot})}\ \ \ \pmb R_{(r_{tot} \times r_{tot})}
\end{pmatrix}
\end{pmatrix}
\]

\(q_{tot} = nq\) and \(r_{tot} = \sum r_i\),
\(\pmb G_{(q_{tot} \times q_{tot})} = diag_{i=1}^n \pmb G_{i (q \times q)}\)
and
\(\pmb R_{(r_{tot} \times r_{tot})} = diag_{i=1}^n \pmb R_{i (r_i \times r_i)}\)
\end{frame}

\begin{frame}{}
\protect\hypertarget{section-3}{}
\begin{itemize}
\item
  Note that \(\pmb R_i\)will often differ between subjects due to
  different numbers of repeated measures (although the underlying
  parameters are usually the same).
\item
  Even when \(\pmb R_i\)or \(\pmb G_i\) are the same across subjects
  (this is usually the case for \(\pmb G_i\)), we keep the subscript
  \(i\) since \(\pmb R\) and \(\pmb G\) are used for complete data form.
  {[}When \(\pmb R_i\) does differ between subjects due to missing data,
  we will later discuss how we can keep dimensions of \(\pmb R_i\)the
  same across subjects and just partition the matrix into `observed' and
  `missing' pieces.{]}
\item
  When \(\pmb G_i\) is the same across subjects, note that
  \(\pmb G_{(q_{tot} \times q_{tot})} = \pmb I_{(n\times n)} \otimes \pmb G_{i(q \times q)}\),
  where \(\otimes\) denotes the \textbf{Kronecker product}. Generally,
  for an \(m \times n\) matrix \(\pmb A\) and \(p \ times q\) matrix
  \(\pmb B\), the Kronecker product is defined as:
\end{itemize}

\[
\pmb  A \otimes \pmb B = 
\begin{pmatrix}
a_{11} \pmb B & a_{12} \pmb B & \dots & a_{1n} \pmb B\\ 
a_{21} \pmb B & a_{22} \pmb B & \dots & a_{2n} \pmb B\\ 
\vdots & \vdots & \ddots & \vdots\\ 
a_{m1} \pmb B & a_{m2} \pmb B & \dots & a_{mn} \pmb B
\end{pmatrix}
\]
\end{frame}

\begin{frame}{}
\protect\hypertarget{section-4}{}
\begin{itemize}
\item
  The normal distribution assumption of the random effects is common.
  There have been methodological developments to account for non-normal
  random effects by considering mixtures of normals (which can yield
  quite a variety of distributions).
\item
  In fitting a linear mixed model with SAS, PROC MIXED, the RANDOM
  statement is used to specify \(\pmb Z\) and \(\pmb G\), while the
  REPEATED statement is used to specify \(\pmb R\). When a REPEATED
  statement is not included, the model will use
  \(\pmb R_i = \pmb I_{r_i}\sigma_\epsilon^2\) (the independent
  structure).
\item
  In modeling a random intercept term by subject, we discussed how the
  following approaches were essentially equivalent (however, see course
  notes for differences in computation between these approaches):\\
  -\alert {random} intercept / subject=id;\\
  -\alert {random} id;
\item
  You can add the option `\(g\)' after the slash in the RANDOM statement
  to get the form and fit for what SAS calls `\(\pmb G\)'.
\end{itemize}
\end{frame}

\begin{frame}{}
\protect\hypertarget{section-5}{}
\begin{itemize}
\item
  The notation for mixed models varies from text to text. We will use
  \(\pmb \beta\) to denote the set of regression coefficients. We can
  denote the set of all covariance parameters in the covariance matrix
  of \(\pmb Y_i\), \((Var[\pmb Y_i]= \pmb V_i)\) as \(\pmb \alpha\).
  Collectively, \(\pmb \theta=(\pmb \alpha,\ \pmb \beta)\) is the set of
  all parameters in a particular mixed model. The variance function
  \(\pmb V_i\) is often written as \(\pmb V_i[\pmb \alpha]\) to indicate
  that all parameters in the matrix involve \(\pmb \alpha\).
\item
  We will typically use \(\pmb b_i\) for random effects. When we have
  only a random intercept, we might call it \(b_i\). If there is a
  random intercept and slope, we can use
  \(\pmb b_i = (\pmb b_{0i},\ \pmb b_{1i})\) (the 1st element for
  intercept, the 2nd for slope).
\item
  We use \(\pmb R_i\) for the `within-subject' covariance matrix and
  \(\pmb G_i\) for the covariance matrix of random effects that
  expresses `between-subject' variability. But note that both of these
  matrices impact \(Var[\pmb Y_i]\), which is the covariance matrix for
  the responses (you could also call it \(Cov[\pmb Y_i]\)).
\end{itemize}
\end{frame}

\begin{frame}{A simple way to account for correlation in an LMM is to
add a `random intercept':}
\protect\hypertarget{a-simple-way-to-account-for-correlation-in-an-lmm-is-to-add-a-random-intercept}{}
\begin{itemize}
\item
  The basic model is
  \(Y_{ij} = \beta_0 + \beta_1 x_{1ij} +\ ...\ + \beta_{p-1} x_{p-1, ij} + b_i + e_{ij} = \pmb X_{ij} \pmb \beta + b_i +\epsilon_ij\)
  where \(\pmb Y\) is the outcome,
  \(\pmb X_{ij} = (x_{1ij},\ ...\ ,\ x_{p–1,ij})\) is a row vector of
  predictors, both for subject \(i\) at time \(j\), and where
  \(\epsilon_{ij} \sim \mathcal N (0,\ \sigma_\epsilon^2)\) and
  \(b_{i} \sim \mathcal N (0,\ \sigma_b^2)\) . These random terms are
  assumed to be independent of each other.
\item
  The main element that distinguishes this from a general linear model
  is the addition of the random term \(b_i\). We also use subject and
  time indices here, with the addition of the repeated measures.
\item
  What is \(Var[\pmb Y_i]\)?
\end{itemize}
\end{frame}

\begin{frame}{A special case of the LMM: a GLM!}
\protect\hypertarget{a-special-case-of-the-lmm-a-glm}{}
\begin{itemize}
\item
  A linear mixed model with no random effects and a simple error
  covariance structure \((\pmb R=\sigma ^2 \pmb I)\) is a general linear
  model.
\item
  For cross-sectional data without correlation, we can fit using SAS
  PROC GLM or the lm function in R. But using LMM software will also
  work! (Like PROC MIXED.)
\item
  For GLM \(\pmb Y= \pmb X \pmb \beta + \pmb \epsilon\), the least
  squares estimator of \(\pmb \beta\) is
  \(\pmb {(X^{\top}X)^{-1}X^{\top}Y}\). To show, apply calculus to
  matrix quantities (see lecture notes). This is also the MLE of
  \(\pmb \beta\). The MLE of \(\sigma^2\) is biased!
\item
  Examples of GLM data
\item
  Myostatin data
\item
  Mouse and smoke data
\end{itemize}
\end{frame}

\begin{frame}{LMM's versus GLM's}
\protect\hypertarget{lmms-versus-glms}{}
\begin{itemize}
\item
  For many simpler models, GLM and LMM modeling approaches will be the
  same (since GLM's are special cases of LMM's).
\item
  Treatment of predictors in LMMs and GLMs are generally the same.
  writing ESTIMATE and CONTRAST statements are similar for GLMs and
  LMMs.
\item
  Inference in the GLM versus LMM
\item
  Both commonly us ML (or REML) to estimate parameters; Wald (\(t\)) and
  LRT (\(F\)) tests are used for testing hypotheses.
\item
  In the GLM, F-tests come via the ANOVA table (algebraic).
\item
  In the LMM, we identify quantities that have approximate \(t\) or
  \(F\) distributions, then calibrate the test by estimating the
  appropriate degrees of freedom.
\end{itemize}
\end{frame}

\begin{frame}{}
\protect\hypertarget{section-6}{}
\begin{itemize}
\item
  There are several (D)DF estimation methods: containment,
  between-within, residual, Satterthwaite, Kenward-Roger. Depending on
  the model, some methods may produce the same (D)DF. One of the key
  issues is accurately estimating the true distribution of the test
  statistic under the null hypothesis.
\item
  In SAS, there are default (D)DF estimation approaches depending on
  whether you have a RANDOM or REPEATED statement (or both). You can
  specify the method you want, or even just numerically specify the DF.
  Recall that both \(t\) and \(F\) distributions are indexed by (D)DF.
\item
  For more details, see Verbeke and Molenberghs, Linear Mixed Models in
  Practice, Springer, 1997, Appendix A, and also the SAS Help
  Documentation.
\end{itemize}
\end{frame}

\begin{frame}{Some simpler LMMs discussed in 6612}
\protect\hypertarget{some-simpler-lmms-discussed-in-6612}{}
\begin{itemize}
\item
  Simple random intercept models (fixed effects plus one random
  intercept for subjects)
\item
  1-sample
\item
  Multi-sample
\item
  For the simple random intercept model, tests for predictors can also
  be conducted via the GLM, using `repeated measures ANOVA'.
\item
  Model with crossed random effects
\item
  Side question: when to make an effect fixed or random?
\item
  Judge data
\item
  Please review the course notes.
\end{itemize}
\end{frame}

\begin{frame}{Important distributions in the LMM}
\protect\hypertarget{important-distributions-in-the-lmm}{}
\begin{itemize}
\tightlist
\item
  The conditional distribution of \(\pmb Y\) given the random effects
  \(\pmb b\)
\end{itemize}

\[
\pmb{Y|b} \sim\mathcal N (\pmb {X\beta} + \pmb {Zb},\ \pmb R) 
= \mathcal N\Big((\pmb X\ \pmb Z) 
\begin{pmatrix}
\pmb \beta\\ \pmb b
\end{pmatrix},\ \pmb R
\Big)
\]

\begin{itemize}
\item
  The classical method to analyze longitudinal data, ``RM ANOVA'',
  essentially makes inference using this conditional distribution, since
  the random effects are treated as fixed effects. Adjustments are then
  made to tests in order to make `correct' inference for estimators that
  account for the clustered data. In some cases this approach may yield
  the same or similar results as fitting a linear mixed model, but
  generally is much more limited.
\item
  The conditional distribution is used to conduct inference for random
  effects using standard LMM methods (i.e., the `Laird and Ware'
  approach).
\end{itemize}
\end{frame}

\begin{frame}{}
\protect\hypertarget{section-7}{}
\begin{itemize}
\item
  The marginal distribution of \(\pmb Y\)
\item
  The joint distribution of \(\pmb Y\) and \(\pmb b\) is
\end{itemize}

\[
\begin{pmatrix}
\pmb Y\\ \pmb b
\end{pmatrix} \sim \mathcal N
\begin{pmatrix}
\begin{pmatrix}
\pmb {X\beta} +\pmb {Zb}\\ \pmb 0
\end{pmatrix},
\begin{pmatrix}
\pmb ZGZ^{\top} +\pmb R\ \ \pmb ZG\\ \pmb G^{\top}Z^{\top}\ \ \ \ \  \pmb G
\end{pmatrix}
\end{pmatrix}
\]

\begin{itemize}
\item
  The marginal distribution of \(\pmb Y\) can be obtained by integrating
  out the random effects \(\pmb b\) from the joint distribution to
  obtain
  \(\pmb Y \sim \mathcal N (\pmb {X\beta},\ \pmb V = \pmb {ZGZ}^{\top} + \pmb R)\).
\item
  The marginal distribution is used in the likelihood functions that are
  used to estimate parameters in the LMM.
\end{itemize}
\end{frame}

\begin{frame}{}
\protect\hypertarget{section-8}{}
\begin{itemize}
\tightlist
\item
  Modern mixed model methodology maximizes the likelihood associated
  with \(\pmb Y\), or it equivalently minimizes
\end{itemize}

\(l = -2ln(L) = r_{tot}ln(2\pi) + ln |\pmb V| + \pmb {(y-X\beta)}^{\top}\pmb V^{-1} \pmb {(y-X\beta)}\)

in order to make inferences about the regression coefficients
\(\pmb \beta\) and covariance parameters \(\pmb \alpha\).

\begin{itemize}
\tightlist
\item
  The likelihood is also often expressed as a combination of
  subject-specific components:
  \(L(\theta)=\prod_{i=1}^n{(2\pi)^{-r_i/2} |\pmb {V_i (\alpha)} |^{-1/2} exp \big(-(\pmb Y_i - \pmb {X_i \beta} )^{\top} \pmb V_i^{-1} (\pmb Y_i - \pmb {X\beta})/2 \big)}\)
\end{itemize}

\(\rightarrow l =-2ln(L)=\sum_{i=1}^n r_i ln(2\pi) + \sum_{i=1}^n ln|\pmb {V_i (\alpha)}| + \sum_{i=1}^n (\pmb {Y_i-X\beta})^{\top} \pmb V_i^{-1} (\pmb \alpha)(\pmb {Y_i-X\beta})\)
\end{frame}

\begin{frame}{General parameter estimation in the mixed model}
\protect\hypertarget{general-parameter-estimation-in-the-mixed-model}{}
\begin{itemize}
\item
  For the standard GLM, there are the regression coefficients
  \(\pmb \beta\) and one covariance parameter \((\sigma^2)\) to
  estimate, which can be carried out using matrix algebra.
\item
  Due to the inclusion of more covariance parameters in the model (in
  either \(\pmb G\) or \(\pmb R\)), parameter estimation in the mixed
  model is not as straightforward and generally requires at least some
  numerical analysis.
\item
  Before describing these techniques in more detail, we will first
  discuss the most common estimation approaches, maximum likelihood (ML)
  estimation and restricted maximum likelihood (REML) estimation. There
  is also the MIVQUE0 estimation approach, which is seldom used.
\end{itemize}
\end{frame}

\begin{frame}{Maximum Likelihood (ML) Estimation}
\protect\hypertarget{maximum-likelihood-ml-estimation}{}
\begin{itemize}
\item
  The ML estimators of \(\pmb \beta\) are obtained by Maximizing the
  likelihood \(L\) or minimizing \(l\) (both given on previous page)
  based on the marginal distribution of \(\pmb Y\). This can be
  accomplished by first noting that Maximizing the likelihood with
  respect to \(\pmb \beta\), conditional on \(\pmb \alpha\), yields:\\
  \(\pmb {\hat \beta(\alpha) = \big( \sum_{i=1}^nX_i^{\top}V_i^{-1}(\alpha)X_i \big)^{-}\sum_{i=1}^nX_i^{\top}V_i^{-1}(\alpha)Y_i}\)
  (subject-specific form)\\
  \(\pmb {\hat \beta(\alpha) = (X^{\top}V^{-1}X)^{-}X^{\top}V^{-1}Y}\)
  (complete-data form) (1)\\
  where \(\pmb V_i = Var[\pmb Y_i] = \pmb {Z_iG_iZ_i^{\top} + R_i}\)
  (subject-specific form).
\item
  Notice that we need values of \(\alpha\) in order to solve (1). To
  accomplish this, we can replace \(\beta\) in the likelihood function
  with its MLE in (1). Now we have a likelihood expressed in terms of
  \(\alpha\) only. Such a likelihood is sometimes referred to as a
  profile likelihood.
\end{itemize}
\end{frame}

\begin{frame}{}
\protect\hypertarget{section-9}{}
\begin{itemize}
\item
  Now we can maximize the profile likelihood function in order to obtain
  using a numerical technique such as a ridge-stabilized Newton-Raphson
  algorithm (common in SAS). We can then go back and determine using (1)
  by replacing \(\pmb \alpha\) with its ML estimates.
\item
  The MLE solution we obtain with this approach is the same as what we
  would obtain if we were able to maximize the likelihood simultaneously
  with respect to \(\pmb \alpha\) and \(\pmb \beta\). Notice that the
  estimator of \(\pmb \beta\) in (1) is identical to the weighted
  least-squares estimates with \(\pmb V^{-1}\) as the weighting matrix.
\item
  One drawback of ML estimation is that associated estimators of
  covariance parameters tend to be biased. REML offers one way to remove
  or reduce bias.
\item
  Note that (1) uses a generalized inverse in case \(\pmb X\) does not
  have full rank. If \(\pmb X\) does have full rank, then we can replace
  \((\pmb {X^{\top}V^{-1}X})^-\) with \((\pmb {X^{\top}V^{-1}X})^{-1}\).
  Issues of model parameterization and estimation here are analogous to
  those discussed in the GLM review.
\end{itemize}
\end{frame}

\begin{frame}{Restricted maximum likelihood (REML) estimation}
\protect\hypertarget{restricted-maximum-likelihood-reml-estimation}{}
\begin{itemize}
\item
  To first introduce REML estimation, consider estimating the population
  variance based on a random sample from the population of interest.
\item
  We know the sample variance (\(s^2\), which uses \(n–1\) in the
  denominator) is unbiased for the population variance in the case that
  the population mean is unknown and estimated (i.e., the usual case).
\item
  But the ML estimator of \(\sigma^2\) has \(n\) in the denominator.
  This demonstrates that ML estimates may not necessarily be unbiased
  estimators. REML estimation offers an alternative to ML estimation
  which helps to circumvent this problem. \textbf{Note: some call
  \(s^2\) the adjusted MLE estimator.}
\end{itemize}
\end{frame}

\begin{frame}{REML estimation for variance}
\protect\hypertarget{reml-estimation-for-variance}{}
\begin{itemize}
\item
  Let \(\pmb J_n = \pmb J_{n \times 1}\) ,
  \(\pmb I_n = \pmb I_{n \times n}\) , and let
  \(\pmb Y=(Y_1,\ Y_2,\ ...\ ,\ Y_n )^{\top}\) , where
  \(\pmb Y \sim \mathcal N(\mu \pmb J_n,\ \sigma^2 \pmb I_n )\)
\item
  Let \(\pmb A\) = any matrix with \(n–1\) independent columns
  orthogonal to \(\pmb J_n\). E.g.:
\end{itemize}

\[
A = 
\begin{pmatrix}
1\ & 0\ & \dots\ & 0\\ -1\ & 1\ & \dots\ & \dots\\ 0\ & -1\ & \dots\ & \dots\\ \dots\ & 0\ & \dots\ & 0\\ \dots\  & \dots\ & \dots\ & 1\\ 0\ & 0\ & \dots\ &  -1
\end{pmatrix}
\]

\begin{itemize}
\item
  Let \(\pmb U= \pmb A^{\top} \pmb Y\) be error contrasts. Note that
  \(\pmb U \sim \mathcal N(0,\ \sigma^2 \pmb {A^{\top} A})\) and that
  \(\sigma^2\) is the only parameter in the distribution for \(\pmb U\).
  Maximizing the likelihood for \(\pmb U\) with respect to \(\sigma^2\)
  yields:
  \(\hat \sigma^2 = \frac {\pmb {Y^{\top} A(A^{\top} A)^{-1} A^{\top} Y}} {n-1}=s^2\).
\item
  In a similar fashion, it can be shown that the REML estimator of
  \(\sigma^2\) in a GLM is \(\frac {\pmb {Y^{\top} (I - P_X)Y}} {n -k}\)
  . Can you do this?
\end{itemize}
\end{frame}

\begin{frame}{REML estimation in the linear mixed model}
\protect\hypertarget{reml-estimation-in-the-linear-mixed-model}{}
\begin{itemize}
\tightlist
\item
  Let \(\pmb A\) be a full rank matrix with columns orthogonal to the
  columns of \(\pmb X\). Then
  \(\pmb U= \pmb {A^{\top} Y} \sim \mathcal N(\pmb 0, \pmb{A^{\top} V(\alpha)A})\),
  which does not depend on \(\beta\). The associated likelihood is
\end{itemize}

\tiny

\(L=(2\pi)^{-(r_{tot}-k)/2} \Big|\sum_{i=1}^n \pmb X_i^{\top} \pmb X_i \Big|^ {1/2} \Big|\sum_{i=1}^n \pmb X_i^{\top} \pmb V_i^{-1} \pmb X_i \Big|^{-1/2} \prod_{i=1}^n |\pmb V_i |^{-1/2} exp \Big(-\frac 1 2 \sum_{i=1}^n (\pmb Y_i-\pmb X_i \pmb{\hat \beta})^{\top} \pmb V_i^{-1} (\pmb Y_i - \pmb X_i \pmb {\hat\beta})\Big)\)

\normalsize

, where \(k=rank(X)\).

\begin{itemize}
\item
  Note that this restricted \(L\) does not involve \(\pmb \beta\)
  parameters (\(\hat \beta\) is a function of \(\pmb \alpha\), as are
  \(\pmb V_i\) matrices) and is not a profile likelihood, as before.
  This is why some software (e.g., SAS) does not penalize for \(\beta\)
  terms in the AIC.
\item
  The restricted likelihood can be maximized to yield \(\alpha\). The
  problem is that this method really only offers a way to estimate
  parameters in \(\alpha\), not \(\pmb \beta\).
\end{itemize}
\end{frame}

\begin{frame}{}
\protect\hypertarget{section-10}{}
\begin{itemize}
\item
  The common approach to estimate \(\pmb \beta\) is then to plug the
  REML estimators of \(\pmb \alpha\) back into equation (1). But
  equation (1) was derived using ML methods, so this estimation of
  \(\pmb \beta\) is really based on a hybrid of ML and REML methods.
  Specifically, estimators of \(\beta\) use the ML form, but employ REML
  estimators of the variance components in that form.
\item
  Verbeke denotes these as ``REML'' estimators of \(\pmb \beta\) (quotes
  emphasized). Since estimation of \(\pmb \beta\) is not based on one
  clear method, some statisticians prefer ML estimation. On the other
  hand, this estimation method does offer a way to reduce bias in
  variance component estimators. Some might argue that this is more
  important than the methodological issue.
\end{itemize}

\begin{block}{Choosing the estimation method in SAS}
\protect\hypertarget{choosing-the-estimation-method-in-sas}{}
\begin{itemize}
\tightlist
\item
  In the PROC MIXED statement, an option can be added: method = ML REML
  MIVQUE0 (no slash to separate the method option from the rest of the
  statement) if ML estimates are of interest. Note that the default
  method is REML; if no option is specified, then REML will be used.
\end{itemize}

Properties of estimators in the LMM: BLUE, BLUP, EBLUE, EBLUP
\end{block}
\end{frame}

\end{document}