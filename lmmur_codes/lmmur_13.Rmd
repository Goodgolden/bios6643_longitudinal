---
title: "lmmur_13"
author: "Randy"
date: "9/18/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("tidyverse")
library("here")
library("janitor")
library("knitr")
library("tinytex")
library("bookdown")
library("nlme")
library("nlmeU")
library("emmeans")
```


## 13.2 The Classical Linear Mixed-Effects Model

### 13.2.1 Specification at a Level of a Grouping Factor

$$
\pmb y_i = \pmb X_i \pmb \beta + \pmb Z_i \pmb b_i + \pmb \epsilon_i, (13.1)
$$

$$
\begin{split}
&\pmb Z_i \equiv 
\begin{pmatrix}
a_{i1}^{(1)}  & a_{i1}^{(2)} & \cdots & a_{i1}^{(q)} \\
\vdots & \vdots & \ddots & \vdots \\
a_{in_i}^{(1)}  & a_{in_i}^{(2)} & \cdots & a_{in_i}^{(q)} \\
\end{pmatrix} 
= (\pmb z_i^{(1)}\  \pmb z_i^{(2)}\ \cdots\ \pmb z_i^{(q)}) ,\\
\\
& \pmb b_i \equiv 
\begin{pmatrix} 
b_{i1}\\
\vdots\\
b_{iq}
\end{pmatrix} \ \ \ \ (13.2)\\
\\
& \pmb b_i \sim \mathcal N_{q} (\pmb 0,\ \pmb {\mathcal D}),\\
\\
& \pmb \epsilon_i \sim \mathcal N_{n_i}(\pmb 0,\ \pmb {\mathcal R}_i)\ \ \ \ (13.3)\\
\\
& \pmb b_i \bot \pmb \epsilon_i
\end{split} 
$$

$$
\begin{split}
\pmb {\mathcal D } = \sigma^2 \pmb D \\
\pmb {\mathcal R_i} = \sigma^2 \pmb R_i
\end{split} \ \ \ \ (13.4)
$$

The representation (13.4), in its general form, is not unique. To make it identifiable, similar to the case of the LM for correlated data (Sect. 10.3), 

we will specify the structure of the matrix $\pmb R_i$ in terms of a set of parameters for a variance function and a correlation matrix (Sect. 13.4.2). 

The specification will imply constraints on $\pmb R_i$ making (13.4) identifiable.


In many cases, the (random) effects included in $\pmb b_i$ have corresponding (fixed) effects, contained in $\pmb \beta$. 

\alert{Consequently, the matrix $\pmb Z_i$ is often created by selecting a subset of appropriate columns of the matrix $\pmb X_i$. In such a situation, it is said that the corresponding fixed and random effects are **coupled**.}


Pinheiro and Bates (2000): **two-level model** a model for data with two levels of grouping, with observations grouped into N first-level groups (indexed by $i = 1,\ ...,\ N)$, each with $n_i$ second-level (sub-)groups (indexed by $j = 1,\ ...,\ n_i$) containing $n_{ij}$ observations, can be written as 

$$
\pmb y_{ij} = \pmb X_{ij} \pmb \beta + \pmb Z_{1,ij}\pmb b_i + \pmb Z_{2,ij} \pmb b_{ij} + \pmb \epsilon_{ij},\  (13.5)
$$ 


with $\pmb b_i \sim \mathcal N_{q_1}(\pmb 0,\ \pmb {\mathcal D}_1)$, 
$\pmb b_{ij} \sim \mathcal N_{q_2}(\pmb 0,\ \pmb {\mathcal D}_2)$, and 
$\pmb \epsilon_{ij} \sim \mathcal N_{n_{ij}} (\pmb 0,\ \pmb {\mathcal R}_{ij})$,
where the random vectors $\pmb b_i$, $\pmb b_{ij}$, and $\pmb \epsilon_{ij}$ are independent of each other. 

In model (13.5), $\pmb b_i$ are the random effects associated with the first-level groups, 
while $\pmb b_{ij}$ are the random effects, independent of the first-level random effects, 
associated with the second-level groups. 

Design matrices $\pmb Z_{1,ij}$ and $\pmb Z_{2,ij}$ can, but do not have to be, identical. 


### 13.2.2 Specification for All Data

$\pmb y \equiv \{_c\ \pmb y_i^{\top} \}_{i=1}^N$ is a vector containing $n=\sum_{i=1}^N n_i$ rows

$\pmb b \equiv \{_c\ \pmb b_i^{\top} \}_{i=1}^N$, and $\pmb \epsilon \equiv \{_c\ \pmb \epsilon_i^{\top} \}_{i=1}^N$ is a vector containing $n=\sum_{i=1}^N n_i$ containing all Nq random effects and n residual errors.

$$
\pmb X_{n \times p} \equiv 
\begin{bmatrix}
\pmb X_1\\
\pmb X_2\\
\vdots\\
\pmb X_N
\end{bmatrix} \ \ \ 
\pmb Z_{n \times Nq} \equiv 
\begin{bmatrix}
\pmb Z_1 & \pmb 0 & \cdots & \pmb 0\\
\pmb 0 & \pmb Z_2 & \cdots & \pmb 0\\
\vdots & \vdots & \ddots & \vdots\\
\pmb 0 & \pmb 0 & \cdots & \pmb Z_N
\end{bmatrix} \ \ \ (13.6)
$$

$$
\begin{split}
\pmb y = \pmb X \pmb \beta + \pmb Z \pmb b + \pmb \epsilon\ \ \ \ (3.17)\\
\pmb b \sim \mathcal N_{Nq}(\pmb 0,\ \sigma^2\pmb D)\\
\pmb \epsilon \sim \mathcal N_n(\pmb 0,\ \sigma^2\pmb R) \ \ \ (13.8)\\
\pmb {\mathbb D} \equiv \pmb I_N \otimes \pmb D = 
\begin{bmatrix}
\pmb D & \pmb 0 & \cdots & \pmb 0\\
\pmb 0 & \pmb D & \cdots & \pmb 0\\
\vdots & \vdots & \ddots & \vdots\\
\pmb 0 & \pmb 0 & \cdots & \pmb D
\end{bmatrix} \\
\pmb R \equiv 
\begin{bmatrix}
\pmb R_1 & \pmb 0 & \cdots & \pmb 0\\
\pmb 0 & \pmb R_2 & \cdots & \pmb 0\\
\vdots & \vdots & \ddots & \vdots\\
\pmb 0 & \pmb 0 & \cdots & \pmb R_N
\end{bmatrix} \ \ \ \ (13.9)
\end{split}
$$

results from the fact that the single-level LMM, defined by (13.1)-(13.4), 
assumes a particular hierarchy of data and random effects, as explicitly shown in (13.3). 

In particular, the model assumes that random effects for different groups, 
defined by levels of a particular factor, are independent. 

Informally, we can describe the hierarchy as generated by grouping factors, with one being nested within the other.

It is possible, however, to formulate random-effects models by using the representation (13.7) with non-block-diagonal matrices $\pmb Z$, $\pmb D$, and $\pmb R$.

This is the case, for instance, of models with crossed random effects.


## 13.3 The Extended Linear Mixed-Effects Model

In some cases, the assumption that the residual errors $\pmb \epsilon_i$ are independent of the random effects $\pmb b_i$,
as specified in (13.3), may be too restrictive. 

For instance, as is done in the mean-variance models, we might postulate that the variance of the residual errors depends on the subject-specific mean value. If we relax the assumption, we obtain an extended LMM. 

$$
\pmb b_i \sim \mathcal N_q(\pmb 0,\ \pmb {\mathcal D}),\ \ and\ \ \pmb \epsilon_i | \pmb b_i \sim \mathcal N_{n_i}(\pmb 0,\ \pmb {\mathcal R}_i) \ \ \ \ (13.10)
$$

$$
\begin{split}
\pmb {\mathcal D } = \sigma^2 \pmb D \\
\pmb {\mathcal R_i} = \sigma^2 \pmb R_i
\end{split} \ \ \ \ (13.4)
$$

with $\pmb {\mathcal D}$ and $\pmb {\mathcal R}_i$ decomposed further as in (13.4). 
We will refer to the above specification as a **hierarchical specification**.

Note that, if we assume that $\pmb \epsilon_i$ in (13.10) is independent of the random effects $\pmb b_i$, then we obtain the classical LMM, specified by (13.1)-(13.4). 

Thus, the extended LMM allows for a more general modeling approach, as compared to the classical LMM.

$$
\pmb b_i \sim \mathcal N_{q_1}(\pmb 0,\ \pmb {\mathcal D}_1),\ \ \  \pmb b_{ij} | \pmb b_i \sim \mathcal N_{q_2}(\pmb 0,\ \pmb {\mathcal D}_2),\  and\  \epsilon_{ij} | \pmb b_i,\ \pmb b_{ij} \sim \mathcal N_{n_{ij}}(\pmb 0,\ \pmb {\mathcal R}_{ij})
$$


## 13.4 Distributions Defined by the $\pmb y$ and $\pmb b$ Random Variables


two continuous random variables $\pmb b$ and $\pmb y$ are described by two probability density functions, which play essential role in defining LMMs. 

The first one is an unconditional distribution of (unobserved) random effects $\pmb b$, defined by (13.8).

The second one is a conditional distribution of the (random) dependent variable $\pmb y$, assuming that random effects are known.


### 13.4.1 Unconditional Distribution of Random Effects

The unconditional distribution $f_b(\pmb b_i)$ of the random effects $\pmb b_i$, defined by (13.3),
is a multivariate normal distribution with zero mean and variance-covariance matrix $\pmb {\mathcal D}$.

Taking into account (13.4), we write 
$$
\pmb {\mathcal D}(\sigma^2,\ \pmb \theta_D) = \sigma^2 \pmb D(\pmb \theta_D) \ \ \ \  (13.11)
$$

where $\pmb \theta_D$ is a vector of parameters, which represent the (scaled by $\sigma^2$) variances and covariances of the elements of $\pmb b_i$.

According to (13.11):

- the matrix $\pmb D$, used to define the variancecovariance matrix of random effects $\pmb b_i$, is parameterized using a vector of parameters $\pmb \theta_D$. 

- it is assumed that any two elements of the vector $\pmb b_i$ can be correlated and there are no restrictions imposed on the matrix $\pmb {\mathcal D}$, except that it is positive-definite and symmetric. 

- In this case, $\pmb D$ has a general structure of a positivedefinite matrix, with $q(q + 1)/2$ distinct elements corresponding to $q$ variances and $q(q âˆ’ 1)/2$ covariances of the random effects included in $\pmb b_i$.

- Consequently, $\pmb \theta_D$ contains $q(q + 1)/2$ distinct parameters. 

- Although $q$ is typically small, estimating all of the parameters may be difficult if, e.g., the sample size $n$ is limited. 

- a simplified structure of the matrix $\pmb {\mathcal D}$ can be chosen (a diagonal form can be assumed, which is equivalent to assuming that all elements of the vector $\pmb b_i$ are independent)

- Plausibility of the assumption will depend on the data at hand. In this case, $\pmb \theta_D$ contains only $q$ distinct parameters.

### 13.4.2 Conditional Distribution of $\pmb y$ Given the Random Effects

$f_{y|b} (\pmb y_i | \pmb b_i)$, of $\pmb y_i$ given $\pmb b_i$ is multivariate normal.

$$
\begin{split}
& E(\pmb y_i| \pmb b_i) \equiv \pmb mu_i = \pmb X_i \pmb \beta + \pmb Z_i \pmb b_i \ \ \ \ (13.12) \\
& Var(\pmb y_i|\pmb b_i) = Var(\pmb \epsilon_i | \pmb b_i) = \sigma^2 \pmb R_i \ \ \ \ (13.13) \\
& with\ \pmb \mu_i \equiv (\mu_{i1},\ ...,\ \mu_{i, n_i})\\
& E(\pmb y_{ij} | \pmb b_i) \equiv \mu_{ij} = \pmb x_{ij}' \pmb \beta + \pmb z_{ij} \pmb b_i \ \ \ (13.14) \\
& where\ \pmb x_{ij} \equiv (x_{ij}^{(1)},\ ...,\ x^{(p)}_{ij})'\\
& and\ \pmb z_{ij} \equiv (z_{ij}^{(1)},\ ...,\ z^{(q)}_{ij})'
\end{split}
$$

- Thus, conditionally on the (unknown) values of the random effects $\pmb b_i$, 

- the mean value of the dependent-variable vector $\pmb y_i$ is defined by a linear combination of the vectors of the X- and Z-covariates included, as columns, 

- in the group-specific design matrices $\pmb X_i$ and $\pmb Z_i$, corresponding to the fixed effects $\pmb \beta$ and random effects $\pmb b_i$, respectively. 

- the conditional variance-covariance matrix of $\pmb y_i$ is equal to the variance-covariance matrix of the residual errors $\pmb \epsilon_i$.


- In their most general form, LMMs are not identifiable, because of the nonuniqueness of the representation (13.4) and because they potentially contain too many unknown parameters

- to make them identifiable, similarly to the matrix $\pmb D$, we can consider representing elements of $\pmb {\mathcal R}_i$ as functions of a limited set of parameters $\pmb \theta_R$, distinct from $\pmb \theta_D$.


For the matrix $\pmb {\mathcal R}_i$, similarly to the approach described in Sect. 10.3 and implemented in R, we could consider the decomposition, given by (10.10), and combine it with the use of variance functions (Sects. 7.2.2 and 7.3.1) and correlation structures (Sect. 10.3.2). 

Thus, $\pmb {\mathcal R}_i$ would become parsimoniously parameterized in terms of a set of parameters of a variance function and a correlation structure. 

In this way not only the number of parameters of the model would be reduced, but the representation (13.4) would become identifiable.


for <$\sigma ,\ \mu $>- and <$\mu $>-groups (Sect. 7.3.1), we follow the **hierarchical specification** (13.10) and apply the
decomposition (10.11) to the conditional distribution of $\pmb \epsilon_i$ given $\pmb b_i$. 

Consequently, we can postulate that 

$$
Var(\epsilon_{ij} | \pmb b_i) = \sigma^2 \lambda^2(\mu_{ij},\ \pmb \delta;\ \pmb v_{ij})\ \ \ \ (13.15)
$$

with $\mu_{ij}$ defined in (13.14). 

It follows that, upon combining the use of the variance function with a correlation structure (Sect. 10.3), 
we can write that

$$
Var(\pmb \epsilon_i | \pmb b_i) = \sigma^2 \pmb R_i(\pmb\mu_i,\ \pmb \theta_R;\ \pmb v_i) \ \ \ \ (13.16)
$$

with $\pmb \theta_R \equiv (\pmb \delta,\ \pmb \rho)$, where $\delta$ is a vector of variance parameters employed by the
variance function $\lambda(Â·)$, $\pmb \rho$ is a vector of parameters related to the chosen correlation structure for the matrix $\pmb R_i$, and $\pmb v_i \equiv (\pmb v_{i1}',\ ...,\ \pmb v'_{i,ni})$ is a vector of variance covariates for the observations from the $i$th group


Equations (13.15) and (13.16) imply that, for models with mean-dependent variance functions from the <$\delta,\ \mu$>- and <$\mu$>-groups (Sect. 7.3.1), $\pmb \epsilon_i$ depend on $\pmb b_i$ through $\mu_i$. 

This violates the assumption of the classical LMM and leads to the extended LMM

- Extended LMMs, defined with the use of variance functions from the <$\delta,\ \mu$>- and <$\mu$>-groups, pose theoretical and computational difficulties (See 13.8 section)

For the mean-independent functions, such as those from the <$\delta$>-group (see Table 7.2), 
the definition (13.16) can be simplified as follows:

$$
Var(\pmb \epsilon_i | \pmb b_i) = Var(\pmb \epsilon_i) = \sigma^2 \pmb R_i (\pmb \theta_R;\ \pmb v_i) \ \ \ \ (13.17)
$$

Note that (13.17) is concordant with the assumption that the residual errors $\epsilon_i$ are independent of the random effects $\pmb b_i$. 

Consequently, the hierarchical model specification with mean-independent variance functions leads to the classical LMM,
with $\pmb {\mathcal R}_i = \sigma^2 \pmb R_i(\pmb \theta_R;\ \pmb v_i)$. 

It is worth noting that the choice of the structure of matrices $\pmb {\mathcal D}$ and $\pmb {\mathcal R}_i$ or,
equivalently, $\pmb D$ and $\pmb R_i$ has consequences for the form of the **marginal variance-covariance matrix** of vector $\pmb y_i$,


### 13.4.3 Additional Distributions Defined by $\pmb y$ and $\pmb b$

additional auxiliary distributions related to LMMs, built on distributions defined in Sects. 13.4.1 and 13.4.2 and play important
role in the various aspects of model fitting and checking model assumptions.

#### 13.4.3.1 Joint Distribution of $\pmb y$ and $\pmb b$

The joint distribution $f_{y,\ b}(\pmb y_i,\ \pmb b_i)$ of $\pmb y$ and $\pmb b$ for the classical LMMs can be specified by taking the product of the unconditional distribution of the random effects $\pmb b$ and the conditional distribution of $\pmb y$ defined in Sects. 13.4.1 and 13.4.2:

$$
f_{y,b}(\pmb y_i,\ \pmb b_i) = f_{y|b}(\pmb y_i | \pmb b_i)f_b(\pmb b_i)
$$

Given that the component distributions, $f_b(\pmb b)$ and $f_{y|b}(\pmb y | \pmb b)f_b(\pmb b)$, are multivariate normal, the joint distribution is also normal. 


#### 13.4.3.2 Marginal Distribution of $\pmb y$


$$
f_y(\pmb y_i) = \int f_{y,b}(\pmb y_i,\ \pmb b_i) d\pmb b_i = \int f_{y|b}(\pmb y_i | \pmb b_i)f_b(\pmb b_i) d\pmb b \ \ \ \ (13.18)
$$

the marginal distribution of $\pmb y$ is also multivariate normal and it can be derived analytically.


#### 13.4.3.3 Posterior Distribution of $\pmb b$ Given $\pmb y$ Is Known

Assuming that the observed values of $\pmb y_i$ are equal to $\pmb y_i^{obs}$, 
the so-called posterior distribution of $\pmb b_i$ conditional on $\pmb y_i^{obs}$ can be calculated using the following general formula:

$$
f_{b|y}(\pmb b_i|\pmb y_i) \equiv f_{b|y} (\pmb b_i| \pmb y_i = \pmb y_i^{obs}) = \frac {f_{y|b}(\pmb y_i|\pmb b_i)f_b(\pmb b_i)} {\int f_{y|b} (\pmb y_i|\pmb b_i) f_b(\pmb b_i)d\pmb b} \ \ \ \ (13.19)
$$


Assuming that the parameters $\pmb \beta$, $\pmb \theta$ are known, 
the posterior distribution $f_{b|y}(\pmb b_i|\pmb y_i)$ for the classical LMMs is multivariate normal. 

Based on the observed data, we often estimate this distribution using its (posterior) mean:


$$
\pmb {\hat b}_i(\pmb \beta,\ \pmb \theta) \equiv \pmb {\hat b}_i = \pmb D \pmb Z'_i \pmb V^{âˆ’1}_i (\pmb y_i^{obs} âˆ’ \pmb X_i\pmb \beta) \ \ \  (13.20)
$$

Since the posterior mean is a linear function of $\pmb y_i$, the variance-covariance matrix of the $\pmb {\hat b_i}$ estimator is equal to

$$
Var(\pmb {\hat b_i}) = 
\sigma^2\pmb D \pmb Z'_i\Bigg\{\pmb V_i^{-1} - \pmb V_i^{-1}\pmb X_i\bigg(\sum_{i=1}^N \pmb X_i'\pmb V_i^{-1}\pmb X_i \bigg)^{-1}\pmb X_i'\pmb V_i^{-1} \Bigg\}\pmb Z_i\pmb D \ \ \ \ (13.21)
$$

To make inference about random effects, we are often interested in assessing the variability of the $\pmb {\hat b}_i âˆ’ \pmb b_i$ difference. The following formula can be used:

$$
Var(\pmb {\hat b}_i âˆ’ \pmb b_i) = \pmb {\mathcal D} âˆ’ Var(\pmb {\hat b}_i) \ \ \ \ (13.22)
$$


It follows from the formula that, for any linear combination of random effects
represented by the column vector $\pmb \lambda$, the following inequality (see (7.7) in Verbeke and Molenberghs 2000) holds:

$$
Var(\pmb \lambda' \pmb {\hat b}_i) \leq Var(\pmb \lambda' \pmb b_i) = \pmb \lambda' \pmb {\mathcal D} \pmb \lambda \ \ \ \ (13.23)
$$


- thhis inequality is one of many ways which illustrate **shrinkage** of the random effects toward the prior mean of $\pmb b_i$, i.e., toward zero. 


- for the LMM defined by (13.1)-(13.4), the posterior mean (13.20) is also the mode of the density of the posterior distribution of $\pmb b_i$, given $\pmb y_i$.


- the use of the mode to predict the random effects can be applied to mixed-effects models in general, including GLMMs and NLMMs. However, for mixed-effects models other than the LMMs (13.1)-(13.4), the mode does not have, in general, to be equal to the posterior mean.


## 13.5 Estimation

The case of the extended, mean-variance model will be discussed separately in Sect. 13.8.


### 13.5.1 The Marginal Model Implied by the Classical Linear Mixed-Effects Model


the classical LMM, Equations (13.12)-(13.13) and (13.17) imply that the marginal mean and variance-covariance matrix of $\pmb y_i$ are given as follows:

$$
\begin{split}
& E(\pmb y_i| \pmb b_i) \equiv \pmb \mu_i = \pmb X_i \pmb \beta + \pmb Z_i \pmb b_i \ \ \ \ (13.12) \\
& Var(\pmb y_i|\pmb b_i) = Var(\pmb \epsilon_i | \pmb b_i) = \sigma^2 \pmb R_i \ \ \ \ (13.13) \\
& Var(\pmb \epsilon_i | \pmb b_i) = Var(\pmb \epsilon_i) = \sigma^2 \pmb R_i (\pmb \theta_R;\ \pmb v_i) \ \ \ \ (13.17)
\end{split}
$$


$$
\begin{split}
& E(\pmb y_i) = \pmb X_i \pmb \beta \ \ \ \ (13.24) \\
& Var(\pmb y_i) \equiv \pmb {\mathcal V}_i(\sigma^2,\ \pmb \theta;\ \pmb v_i) \\
& = \sigma^2 \pmb V_i(\pmb \theta;\ \pmb v_i) = \sigma^2[\pmb Z_i \pmb D(\pmb \theta_D)\pmb Z_i' + \pmb R_i(\pmb \theta_R;\ \pmb v_i)] \ \ \ \  (13.25)\\
& \pmb y_i \sim \mathcal N_{n_i} (\pmb X_i \pmb \beta,\ \sigma^2 \pmb Z_i \pmb D \pmb Z_i' + \sigma^2 \pmb R_i) \ \ \ \ (13.26)
\end{split}
$$

The marginal mean value of the dependent variable vector $\pmb y_i$, similarly to the linear model (10.1)-(10.5), is defined by a linear combination of the vectors of covariates included, as columns, in the group-specific design matrix $\pmb X_i$, with parameters $\pmb \beta$.

Hence, strictly speaking, the model employing random effects, specified in (13.1)-(13.4), implies a marginal normal distribution,
defined by (13.26), which is similar to distributions considered in Chap. 10 in the context of LMs for correlated data, but with the variance-covariance matrix of $\pmb y_i$ of a very specific parametric form, given by (13.25).


- the marginal model, defined by (13.24)-(13.26), does not involve the random effects $\pmb b_i$. 

- the matrix $\pmb {\mathcal D}$ does not have to be treated as a variance-covariancematrix. 

- it does not have to be positive-definite, as long as the matrix $\pmb {\mathcal V}_i$ is positive-definite. 

- the matrix $\pmb {\mathcal D}$ does need to be symmetric, though, to assure that the matrix $\pmb {\mathcal V}_i$ is symmetric. 

- while every LMM of the form, specified in (13.1)-(13.4), implies a marginal model, defined by (13.26),
not every model of the form (13.26) can be interpreted as resulting from an LMM.

- thus, the two models are not equivalent.


\alert {LMs with fixed effects and correlated residual errors, presented in Chap. 10, are less restrictive than LMMs.}

- the former are more flexible than the latter. 

- On the other hand, in general, LMs with fixed effects and correlated residual errors do not allow making inference about the
variability that may be related to different levels of the data hierarchy.

- the effects of the covariates, included in the design matrix $\pmb X_i$, are quantified by the same parameters $\pmb \beta$ in both the conditional (13.12) and unconditional (13.24) mean. 

- thus, although the parameters are defined in the context of the subject-specific model (13.1), they can also be interpreted as quantifying effects at the population level. 

- this possibility of a dual interpretation of fixed-effects $\pmb \beta$ is **a unique feature of the classical LMM**, given by (13.1)-(13.4).

- it does not hold, for instance, for GLMMs, not described in this book.

- the fact that the classical LMM implies the marginal model (13.26) is also important from a practical point of view. 

- because it allows the construction of effective estimation approaches for the LMM. 


### 13.5.2 Maximum-Likelihood Estimation


the ML estimation involves constructing the likelihood function based on appropriate probability distribution function for the observed data.


**The unconditional distribution of $f_b$ and the conditional distribution of $f_{y|b}$, which were defined in Sect. 13.4 for the classical LMM, are not suitable for constructing the likelihood function, because the random effects $\pmb b_i$ are not observed.**

**for a similar reason, the joint distribution cannot be used.**

Instead, estimation of LMMs is based on the marginal distribution of $\pmb y_i$, which coincides with the distribution given in (13.26).


$$
\begin{split}
& E(\pmb y_i) = \pmb X_i \pmb \beta \ \ \ \ (13.24) \\
& Var(\pmb y_i) \equiv \pmb {\mathcal V}_i(\sigma^2,\ \pmb \theta;\ \pmb v_i) \\
& = \sigma^2 \pmb V_i(\pmb \theta;\ \pmb v_i) = \sigma^2[\pmb Z_i \pmb D(\pmb \theta_D)\pmb Z_i' + \pmb R_i(\pmb \theta_R;\ \pmb v_i)] \ \ \ \  (13.25)\\
& \pmb y_i \sim \mathcal N_{n_i} (\pmb X_i \pmb \beta,\ \sigma^2 \pmb Z_i \pmb D \pmb Z_i' + \sigma^2 \pmb R_i) \ \ \ \ (13.26)
\end{split}
$$

- the estimation of parameters of the classical LMM can be accomplished by using the ML or REML estimation for the implied marginal model, along the lines similar to those described in Sect. 10.4.2.

$$
l_{Full}(\pmb \beta,\ \sigma^2,\ \pmb \theta) \equiv âˆ’ \frac N 2 \log(\sigma^2) âˆ’ 
\frac 1 2 \sum^N_{i=1} \log[det(\pmb V_i)]
âˆ’ \frac 1 {2\sigma^2} \sum^N_{i=1} (\pmb y_iâˆ’\pmb X_i\pmb \beta)' \pmb V^{âˆ’1} _i (\pmb y_i âˆ’ \pmb X_i \pmb \beta) \ \ \ \ (13.27)
$$

where $\pmb V_i$ depends on $\pmb \theta$

(using a log-profile-likelihood) log-profile-likelihood results from plugging into (13.27) the
estimators of $\pmb \beta$ and $\sigma^2$, 
given by

$$
\pmb {\hat \beta}(\pmb \theta) \equiv \bigg(\sum_{i=1}^N \pmb X'_i \pmb V^{âˆ’1}_i \pmb X_i\bigg)^{âˆ’1} \sum_{i=1}^N \pmb X'_i \pmb V^{âˆ’1}_i\pmb y_i \ \ \ \ (13.28)
$$

$$
\hat \sigma^2_{ML}(\pmb \theta) \equiv \sum^N_{i=1} \pmb r'_i \pmb V^{âˆ’1}_i \pmb r_i/n \ \ \ \ (13.29)
$$

$\pmb r_i \equiv \pmb r_i(\pmb \theta) = \pmb y_i âˆ’ \pmb X_i \pmb {\hat \beta}(\pmb \theta)$. 

Note that the expressions correspond to (10.26) and (10.27), presented in Sect. 10.4.2. 

By maximizing the log-profile-likelihoodfunction over $\pmb \theta$, we obtain estimators of these parameters. 

Plugging $\pmb {\hat \theta}$ into (13.28) and (13.29) yields the corresponding estimators of $\pmb {\hat \beta}$ and $\hat \sigma^ 2$, respectively.

the ML estimates of the variance-covariance parameters are biased. 

For this reason, the parameters are better estimated using the REML estimation. 

Toward this end, the log-restricted-likelihood function, corresponding to (10.30), is considered.


$$
\hat \sigma^2_{REML}(\pmb \theta) \equiv
\sum^N_{i=1} \pmb r'_i \pmb V^{âˆ’1}_i \pmb r_i / (n âˆ’ p) \ \ \ \ (13.30)
$$

$$
l^*_{REML}(\pmb \theta) \equiv  âˆ’ \frac {n âˆ’ p} 2 
\log \sum _{i=1}^N \bigg(\pmb r'_i \pmb r_i\bigg) âˆ’ \frac 1 2 \sum _{i=1}^N \log[det(\pmb V_i)]
âˆ’ \frac 1 2 \log \bigg[ det \bigg(\sum_{i=1}^N \pmb X_i \pmb V^{âˆ’1}_i \pmb X_i\bigg)\bigg] \ \ \ \  (13.31)
$$

Maximization of (13.31) yields an estimator of $\pmb \theta$, which is then plugged into (13.28) and (13.30) to provide estimators of $\pmb \beta$ and $\sigma^2$, respectively.


### 13.5.3 Penalized Least Squares

Essentially, the approach is based on the log-profile-restricted-likelihood for $\pmb \theta$, as defined in Sect. 13.5.2. 

However, the numerical algorithm based on sparse matrices allows for a numerically efficient implementation of this penalized least squares (PnLS) approach. 

In our presentation we follow Bates (2012) who describes in detail a more general version of this algorithm, namely, penalized weighted least squares (PWLS) used in the context of GLMMs and NLMMs and implemented in the package lme4.0. 


- a single-level LMM, specified for all data (Sect. 13.2.2)

- assume the conditional independence model and homogeneous residual-error variance, i.e., $\pmb R \equiv \pmb I_n$.


for the PnLS estimation approach, the starting point is the density of the joint distribution of $\pmb y$ and random effects $\pmb b$ introduced in general terms in Sect. 13.4.3.

$$
h_{Joint}(\pmb y,\ \pmb b;\ \pmb \beta,\ \sigma^2,\ \pmb \theta) \equiv 
âˆ’ \frac {n + Nq} 2 \log(\sigma^2) âˆ’ \frac 1 2 \log[det(\pmb {\mathbb D})]
âˆ’ \frac {(\pmb y âˆ’ \pmb X \pmb \beta âˆ’ \pmb Z\pmb b)' (\pmb y âˆ’ \pmb X \pmb \beta âˆ’ \pmb Z\pmb b) + \pmb b'\pmb {\mathbb D}^{âˆ’1}\pmb b} {2\sigma^2}  \ \ \ \ (13.32)
$$

$$
\begin{split}
& \pmb {\mathbb D} \equiv \pmb I_N \otimes \pmb D = 
\begin{bmatrix}
\pmb D & \pmb 0 & \cdots & \pmb 0\\
\pmb 0 & \pmb D & \cdots & \pmb 0\\
\vdots & \vdots & \ddots & \vdots\\
\pmb 0 & \pmb 0 & \cdots & \pmb D
\end{bmatrix}\ \ \ \ (13.9) \\
& \pmb R \equiv \pmb I_n,\ given\ assumption \\
& \pmb \theta \equiv \pmb \theta_D\\
& \pmb {\mathbb D} = \pmb {TSST}' (Cholesky)\ \ \ \ \ (13.33)
\end{split}
$$

where $\pmb T$ is a lower-triangular matrix with all diagonal elements equal to 1

$\pmb S$ is a diagonal matrix with nonnegative diagonal elements, we can express $\pmb b$ as follows:

$$
\pmb b = \pmb T \pmb S \pmb u,\ \ \  with\  \pmb u \sim \mathcal N_{Nq}(\pmb 0,\ \sigma^2\pmb I_{Nq}).
$$ 

By allowing for zero elements on the diagonal matrix $\pmb S$ used in (13.33), 
we consider a general case with a potentially singular (positive semi-definite) matrix $\pmb {\mathbb D}$.


It follows that, conditionally on $\pmb u$, $\pmb y$ is normally distributed with

$$
\begin{split}
& E(\pmb y | \pmb u) = \pmb X\pmb \beta + \pmb Z \pmb T\pmb S \pmb u 
\equiv 
\pmb X \pmb \beta + \pmb A' \pmb u\\
& Var(\pmb y | \pmb u) = \sigma^2 \pmb I_n
\end{split} \ \ \ \ \ (13.34)
$$

while the marginal mean and variance of $\pmb y$ can be expressed as

$$
\begin{split}
& E(\pmb y) = \pmb X\pmb \beta \\
& Var(\pmb y) = \sigma^2(\pmb A'\pmb A + \pmb I_n)
\end{split} \ \ \ \ \ (13.35)
$$



Using the representation introduced above, (13.32) can be written as follows:

$$
h_{PnLS}(\pmb y,\ \pmb b;\ \pmb \beta,\ \sigma^2,\ \pmb \theta) 
\equiv âˆ’ \frac {n+Nq} 2 \log(\sigma^2) âˆ’ \frac {(\pmb y âˆ’ \pmb X\pmb \beta âˆ’ \pmb A'\pmb u)' (\pmb y âˆ’ \pmb X\pmb \beta âˆ’ \pmb A'\pmb u) + \pmb u'\pmb u} {2 \sigma^2}
\equiv âˆ’ \frac {n+Nq} 2 \log(\sigma^2)âˆ’ \frac {d(\pmb \beta,\ \pmb \theta)} {2\sigma^2} \ \ \ \ \ (13.36)
$$

Note that term $d(\pmb \beta,\ \pmb \theta)$ in (13.36) resembles a penalized sum of squares. 

In fact, it can be seen as a residual sum of squares in a linear regression model

$$
E
\begin{pmatrix}
\pmb y\\ 
\pmb 0
\end{pmatrix}
= 
\begin{bmatrix}
\pmb A'& \pmb X\\
\pmb I_{Nq} & \pmb 0
\end{bmatrix}
\begin{pmatrix}
\pmb u\\
\pmb \beta
\end{pmatrix}
\equiv \pmb X^âˆ—
\begin{pmatrix}
\pmb u\\
\pmb b
\end{pmatrix}
$$

The solution $(\pmb {\tilde u},\ \pmb {\tilde b})$ for the linear regression problem satisfies

$$
(\pmb X^âˆ—)' \pmb X^âˆ—
\begin{pmatrix}
\pmb {\tilde u}\\
\pmb {\tilde \beta}
\end{pmatrix}
= (\pmb X^âˆ—)'
\begin{pmatrix}
\pmb y\\
\pmb 0
\end{pmatrix}
$$

$$
\begin{bmatrix}
AA' + I_{Nq} & AX\\
X'A' & X'X
\end{bmatrix}
\begin{pmatrix}
\tilde u\\
\tilde \beta
\end{pmatrix}=
\begin{pmatrix}
Ay\\
X'y
\end{pmatrix} \ \ \ \ (13.37)
$$

It is worth noting that (13.37) corresponds to a general form of the LMM equations
considered by Henderson (1984), which allow for a singular estimate of $\pmb {\mathbb D}$

a sparse lower-triangular Cholesky decomposition matrix

$$
L = 
\begin{bmatrix}
L_z & 0\\
L_{ZX} & L_X
\end{bmatrix},\ \ LL' = P(X^*)'X^*P' \ \ \ \ \ (13.38)
$$

- the orthogonal matrix $P$ is a **fill-reducing permutation matrix**, determined from the pattern of nonzero elements in $Z$. 

- the matrix reduces the number of nonzero elements in $L$ and hence has a large impact on the storage space required for $L$. 

- it is important to stress that, although this has not been explicitly indicated in
(13.38), $L$ depends on $\theta$.

- If we assume that the matrix $P$ is of a block-diagonal form:

$$
P = 
\begin{bmatrix}
P_Z & 0 \\
0 & P_X
\end{bmatrix},\ 
\begin{bmatrix}
AA' + I_{Nq} & AX\\
X'A' & X'X
\end{bmatrix} =
\begin{bmatrix}
P'_ZL_Z & 0\\
P'_XL_{ZX} & P_X'L_X
\end{bmatrix}
\begin{bmatrix}
L_Z'P_Z & L_{ZX}'P_{X}\\
0 & L_X'P_X
\end{bmatrix} \ \ \ \ \ (13.39)
$$


$$
h_{PnLS}(\pmb y,\ \pmb b;\ \pmb \beta,\ \sigma^2,\ \pmb \theta) 
\equiv âˆ’ \frac {n+Nq} 2 \log(\sigma^2) âˆ’ \frac {(\pmb y âˆ’ \pmb X\pmb \beta âˆ’ \pmb A'\pmb u)' (\pmb y âˆ’ \pmb X\pmb \beta âˆ’ \pmb A'\pmb u) + \pmb u'\pmb u} {2 \sigma^2}
\equiv âˆ’ \frac {n+Nq} 2 \log(\sigma^2)âˆ’ \frac {d(\pmb \beta,\ \pmb \theta)} {2\sigma^2} \ \ \ \ \ (13.36)
$$

rewrite (13.36) as follows:

$$
h_{PnLS}(\pmb y,\ \pmb b;\ \pmb \beta,\ \sigma^2,\ \pmb \theta) =
 âˆ’ \frac {n+Nq} 2 \log(\sigma^2)âˆ’ \frac {\tilde d( \pmb \theta)} {2\sigma^2}
 - \frac 1 {2\sigma^2} 
\begin{pmatrix}
\pmb P_Z(\pmb u - \pmb {\tilde u})\\
\pmb P_X(\pmb \beta - \pmb {\tilde \beta})
\end{pmatrix}'
\pmb {LL}'
\begin{pmatrix}
\pmb P_Z(\pmb u - \pmb {\tilde u})\\
\pmb P_X(\pmb \beta - \pmb {\tilde \beta})
\end{pmatrix} \ \ \ \ (13.40)
$$

where $\tilde d(\pmb \theta)$ is the value of penalized sum of squares $d(\pmb\beta,\ \pmb \theta)$, defined in (13.36), computed at solution $(\pmb {\tilde u},\ \pmb {\tilde \beta})'$ of system of (13.37). 

Thus, $\tilde d(\pmb \theta)$ is the minimum value of penalized sum of squares, assuming $\pmb \theta$ is known.

$$
\begin{split}
& l_{ML}(\pmb \beta,\ \sigma^2,\ \pmb \theta) \equiv
âˆ’ \frac n 2 \log(\sigma^2) âˆ’ \frac 1 2
\log\{[det(\pmb L_Z)^2]\} âˆ’ \frac {\tilde d(\pmb \theta)} {2\sigma^2}
- \frac 1 {2\sigma^2} [\pmb L'_X\pmb P_X(\pmb \beta - \pmb {\tilde \beta})]'\pmb L'_X\pmb P_X(\pmb \beta - \pmb {\tilde \beta})
\ \ \ \ (13.41)\\
& \tilde \sigma^2_{ML} \equiv \frac {\tilde d(\pmb \theta)} n \ \ \ \ (13.42)\\
& l_{ML}^*(\pmb \theta) \equiv
âˆ’ \frac 1 2
\log\{[det(\pmb L_Z)]^2\} âˆ’ \frac n 2 \log [\tilde d(\pmb \theta)] \ \ \ \ \ (13.43)
\end{split}
$$

$$
l_{Full}(\pmb \beta,\ \sigma^2,\ \pmb \theta) \equiv âˆ’ \frac N 2 \log(\sigma^2) âˆ’ 
\frac 1 2 \sum^N_{i=1} \log[det(\pmb V_i)]
âˆ’ \frac 1 {2\sigma^2} \sum^N_{i=1} (\pmb y_iâˆ’\pmb X_i\pmb \beta)' \pmb V^{âˆ’1} _i (\pmb y_i âˆ’ \pmb X_i \pmb \beta) \ \ \ \ (13.27)
$$

The log-profile-likelihood is a re-parameterized version of the function obtained from plugging the estimators (13.28) and (13.29) into (13.27)


$$
l_{REML}^*(\pmb \theta) \equiv
âˆ’ \frac 1 2
\log\{[det(\pmb L_Z)det(\pmb L_X)]^2\} âˆ’ \frac {n-p} 2 \log [\tilde d(\pmb \theta)] \ \ \ \ \ (13.44)
$$

$$
\tilde \sigma^2_{REML} \equiv \frac {\tilde d(\pmb \theta)} {n-p} \ \ \ \ (13.45)
$$

the estimates from (13.37)

$$
\begin{bmatrix}
AA' + I_{Nq} & AX\\
X'A' & X'X
\end{bmatrix}
\begin{pmatrix}
\tilde u\\
\tilde \beta
\end{pmatrix}=
\begin{pmatrix}
Ay\\
X'y
\end{pmatrix} \ \ \ \ (13.37)
$$

First, the decomposition (13.33) of the matrix $\pmb {\mathbb D}$ has been replaced by the classical Cholesky decomposition $\pmb {\mathbb D} = \pmb {\mathbb Q}\pmb {\mathbb Q}'$. 

Additionally, the lower-triangular matrix on the right-hand side of (13.39) has been assumed to take the following form:

$$
\begin{bmatrix}
P'_ZT_Z & 0 \\
T_{ZX}' & T'_X
\end{bmatrix}
$$

where matrices $T_Z$ (lower-triangular), $P_Z$ (permutation), $T_X$, and $T_{ZX}$ (uppertriangular) are defined by the following relationships:

$$
\begin{split}
& T_Z T'_Z \equiv P_Z(AA' + I_{Nq})P'Z, \\
& T_ZT_{ZX} \equiv  P_Z AX, \\
& T'_{ZX}T_{ZX} \equiv  X'X âˆ’ T'_XT_X
\end{split}
$$

with $A \equiv \mathbb Q'Z'$. 

the log-profile-likelihood and log-profile-restricted-likelihood similar to (13.43) and (13.44), respectively, 

but with $det(L_Z)$ and $det(L_X)$ replaced, respectively, by $det(T_Z)$ and $det(T_X)$. 

Equation (13.37), defining the PnLS estimates $\tilde u$ and $\tilde b$, can now be equivalently expressed as

$$
\begin{split}
& T_X \tilde \beta = c_\beta,\\
& T'_ZP_Z\tilde u = c_u âˆ’ T_{ZX}\tilde \beta,\\
& T_Zc_u = P_ZAy, \\
& T'_Xc_\beta = X'y âˆ’ T_{ZX}c_u.
\end{split}
$$


\alert{13.5.4, 13.5.5, and 13.5.6 ignored}


## 13.6 Model Diagnostics

### 13.6.1 Normality of Random Effects

$$
\hat b_i \equiv \hat D Z_i' V_i^{-1} (y_i ^{(obs)} âˆ’ X_i\beta) \ \ \ \  (13.50)
$$

The conditional expectations are often called empirical Bayes (EB) estimates,
because they are obtained by using the estimated values of the fixed parameters $\pmb \beta$
and variance-covariance parameters $\pmb \theta$ in (13.20). 

$$
\pmb {\hat b}_i(\pmb \beta,\ \pmb \theta) \equiv \pmb {\hat b}_i = \pmb D \pmb Z'_i \pmb V^{âˆ’1}_i (\pmb y_i^{obs} âˆ’ \pmb X_i\pmb \beta) \ \ \  (13.20)
$$

Note that, strictly speaking, the random effects $b_i$ are not parameters, so that rather than estimating their values, we are predicting them.

the conditional expectations (13.50) might be called **predictors**. In fact, they are often referred to as **best linear unbiased predictors (BLUPs)** or **empirical BLUPs (EBLUPs)**. 

This term follows from the fact that it can be shown that the conditional expectations are **BLUPs** of $b_i$ in the sense that they are unbiased and have minimum variance among all unbiased estimators, which are linear combinations of $y_i$. 

Similarly to (13.23), shrinkage of **EBLUPs** can be illustrated by noting that the
following inequality, 

$$
var(\pmb \lambda' \pmb {\hat b_i}) \leq \pmb \lambda' \pmb {\hat D} \pmb \lambda \ \ \ \  (13.51)
$$

It appears that using histograms or Q-Q plots of the predicted random errors for the purpose of checking their normality is of limited value. 

That is because the observed distribution of $\hat b_i$ does not necessarily reflect the true distribution of $b_i$


However, the plots of the conditional modes can be used to detect, e.g., outlying values that might warrant further inspection. 

if the histogram is, e.g., bimodal, it may indicate that a covariate has been omitted from the $\pmb Z_i$ matrix.


based on the comparison of the results obtained for a LMM with and without assuming the normality

however, that if the inferential goal focuses on the marginal model (13.26), and especially on the fixed effects, valid inference can be obtained even if the random effects do not follow a normal distribution

### 13.6.2 Residual Diagnostics

given the structure of the classical LMM, defined in (13.1)-(13.4), various types of raw residuals can be defined.

One set is the conditional residuals, which follow from the conditional mean representation (13.12), and are defined as

$$
\pmb {\hat \epsilon}_{(c)i} \equiv 
\pmb y_i âˆ’ \pmb X_i \pmb {\hat \beta} âˆ’ \pmb Z_i \pmb {\hat b}_i \ \ \ \ (13.52)
$$ 

where the formula for $\pmb {\hat b}_i$ is given in (13.50).


Another set is the marginal residuals, resulting from the marginal mean representation, given by (13.24). 

The marginal residuals are defined as

$$
\pmb {\hat \epsilon}_{(m)i} 
\equiv \pmb y_i âˆ’ \pmb X_i \pmb \beta \ \ \ \  (13.53)
$$


- the raw residuals are useful to check heterogeneity of the conditional or marginal variance

- they are less recommended, however, for checking normality assumptions and/or detecting outlying observations

- raw residuals will be correlated and their variances will differ

- the studentized and Pearson residuals are more often used (see Sects. 4.5.1 and 7.5). 

- as in the case of the LM for correlated data (see Sect. 10.5), even the scaled residuals are not appropriate for, e.g., checking the normality of the residual errors

- because the model (13.1)-(13.4) allows for a correlation between the errors

- approximate solution is to consider the transformation of the raw conditional or marginal residuals, which were defined in (13.52) and (13.53), respectively, based on the Cholesky decomposition of the (estimate of) residual variance-covariance matrix $\sigma^2 \pmb R_i$ or the marginal variance-covariance matrix $\sigma^2 \pmb V_i$, respectively (see Sects. 4.5.1 and 10.5).

$$
\begin{split}
\hat \epsilon_{(c)i}^*
\equiv (\hat \sigma \hat U'_{(c)i}) ^ {âˆ’1}\hat \epsilon_{(c)i} \ \ \ \ \ (13.54)\\
\hat \epsilon_{(m)i}^* 
\equiv (\hat \sigma U'_{(m)i})^{âˆ’1}\hat \epsilon_{(m)i} \ \ \ \ \ (13.55)\\
\hat U'_{(c)i} \hat U_{(c)i} = \pmb R_i \\
\hat U'_{(m)i} \hat U_{(m)i} = \pmb V_i,
\end{split}
$$

Then, $\hat \epsilon^*_{(c)i}$ (Pinheiro and Bates 2000, pp. 239) and $\hat \epsilon^*_{(m)i}$ (Schabenberger 2004) should be approximately normally distributed with mean zero and variance-covariance matrix equal to an identity matrix.

the scatterplot of the residuals against the estimated marginal mean values
can be used to detect patterns suggesting a possible problem in the specification of
the mean structure of the data or to check for outliers.

**in nlme** the transformed conditional residuals are available.

the marginal residuals are pure, in the sense that the residuals are a function of only the marginal errors, which they are supposed to estimate. 


On the other hand, the conditional residuals, which estimate the residual errors, are confounded with the
random effects, because the residuals are a function of $b_i$ and $\epsilon_i$.


### 13.6.3 Influence Diagnostics

The basic tool to investigate the influence of a given observation on the estimates is the likelihood displacement. 

It was introduced in the context of the classical LM in Sect. 4.5.3. 

Recall that the likelihood displacement, $LD_i$, as in (4.27), is defined as the change between the maximum log-likelihood computed when using all data and when excluding the $i$-th observation. 

For the LMM, given by (13.1)-(13.4), the likelihood-displacement definition (4.27) is modified by
specifying $\hat \Theta \equiv (\pmb {\hat \beta}',\ \pmb {\hat \theta}',\ \sigma^2)'$ and using the log-likelihood (13.27).










