---
title: "lmmur_r09"
author: "Randy"
date: "10/9/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Chapter 9 ARMD Trial: Linear Model with Heterogeneous Variance

In the current chapter, we allow for heterogeneous variance, but we keep the assumption of independence. Note that the results of the exploratory analysis indicate that the assumption is most likely incorrect.

## 9.2 A Linear Model with Independent Residual Errors and Heterogeneous Variance

$$
VISUAL_{it} = \beta_{0t} + \beta_1 \times VISUAL0_i + \beta_{2t} \times TREAT_i + \epsilon_{it} \ \ \ \ (9.1)
$$

Note that the exploratory analysis of the ARMD data (Sect. 3.2.2) indicated that the variances of the visual acuity measurements, obtained at different timepoints, differed.

we allow for heteroscedasticity: $\epsilon_{it} \sim \mathcal N(0,\ \sigma^2_t) \ \ \ (9.2)$

$$
Var(VISUAL_{it}) \equiv \sigma_t^2 = 
\begin{cases}
\sigma^2 & for\ t=1\ (4\ weeks)\\
\sigma^2\delta_2^2 & for\ t=2\ (12\ weeks)\\
\sigma^2\delta_3^2 & for\ t=3\ (24\ weeks)\\
\sigma^2\delta_4^2 & for\ t=4\ (52\ weeks)
\end{cases}
$$

where $\delta_2 \equiv \sigma_2/ \sigma_1$, $\delta_3 \equiv \sigma_3/ \sigma_ 1$, and $\delta_4 \equiv \sigma_ 4/\sigma_ 1$. Thus, parameters $\delta_2$, $\delta_3$, and $\delta_4$, are the ratios of standard deviations (SDs) of visual acuity measurements for weeks 12, 24, and 52, relative to SD at week 4 (the reference level).

Note that, according to (9.3), the scale parameter $\sigma$ can be interpreted as SD at 4 weeks.

### 9.2.1 Fitting the Model Using the gls() Function

To allow for heterogeneous variance, we set the **weights argument** to an object of the `varIdent` class, created with the help of the **varIdent()** constructor function. The `varIdent` class represents a variance structure with different variances for different strata.

In our case, the strata are defined by the levels of the factor **time.f**, which is indicated using the formula **\~1\|time.f** in the form argument of the **varIdent()** constructor function.

```{r}
## (a) Selected results for the model 
## with timepoint-specific variances
data(package = "nlmeU", armd)
lm1.form <- # See also R6.1
  formula(visual ~ -1 + visual0 + time.f + treat.f:time.f)

# Var. function; <delta>-group
fm9.1 <- # M9.1
 gls(lm1.form,
     weights = varIdent(form = ~1|time.f), 
     data = armd)
summary(fm9.1)
str(fm9.1)

## the delta1 = 1, delta2, delta3, delta4
## for the variancec structure
fm9.1$modelStruct$varStruct 

(intervals(fm9.1, which = "var-cov")) 

## (b) REML-based LR test of homoscedasticity. 
## The object fm6.1 was created in Panel R6.3
# anova(fm6.1, fm9.1) # M6.1 \in M9.1
```

By referring to the varStruct component of the modelStruct component of the model-fit object (Table 8.2a), we display the estimated values of the $\delta$ variance-function coefficients.

-   the estimates indicate an increasing variability of outcome measurements in time.

-   by using the `intervals()` function (Table 8.2a), we obtain the estimates and 95% confidence intervals for the parameters $\delta$ and $\sigma$ .

-   the 95% confidence intervals for the variance function coefficients slightly overlap, but suggest timepoint-specific variances.

-   Note that both models differ only by their variance structure and were fitted using the `gls()` function with the default estimation method, i.e., REML.

-   there is a statistically significant difference, the data provide evidence for heterogeneous variances of the outcome.

-   in further modeling, we will therefore assume heteroscedasticity

## 9.3 Linear Models with the `varPower(·)` Variance-Function

The mean structure for all models introduced in this section is the same as the one specified in (9.1). The models differ with respect to the assumed form of the variances of the residual errors.

M9.2 with strata for different groups

$$
\sigma_{it} = \sigma \lambda_{it} = \sigma \lambda (\delta;\ TIME_{it}) 
= \sigma (TIME_{it})^\delta \ \ \ \ \ (9.4)
$$

the `varPower(·)` variance function from the $<\delta>$-group (Table 7.2), with $\delta \equiv \delta$, variance covariate $TIME_{it}$, and no strata.

It specifies that the variance is a power function of the time (in weeks), at which the visual acuity measurement was taken.

M9.3 assmues the power coefficient depends on treatment:

$$
\sigma_{it} = \sigma \lambda _{it} = \sigma \lambda\{(\delta_1,\ \delta_2);\  TIME_{it}\} = 
\begin{cases}
\sigma(TIME_{it})^{\delta_1} & for Active \\
\sigma(TIME_{it})^{\delta_2} & for Placebo 
\end{cases}
\ \ \ \ \ (9.5)
$$

M9.4 specifies that the variances are a power function of the mean value:

$$
\sigma_{it} = \sigma \lambda_{it} = \sigma \lambda(\mu_{it},\ \delta) = \sigma (\mu_{it} )^\delta\ \ \ \ \ (9.6)
$$

where $\mu_{it} \equiv \beta_{0t} + \beta_1 \times VISUAL_{0i} + \beta_{2t} \times TREAT_i$ is the predicted (mean) value of $VISUAL_{it}$, as implied by (9.1). The function $\lambda(·)$, used in (9.6), is an example of the `varPower(·)` variance function from the $<\delta,\ \mu>$-group (Table 7.3), with $\pmb \delta \equiv \delta$ and strata.

Finally, M9.5 assumes a constant coefficient of variation, i.e., it assumes that SDs of the visual acuity measurements are proportional to the mean values:

$$
\sigma_{it} = \sigma \lambda_{it} = \sigma \lambda (\mu_{it}) = \sigma \mu_{it} \ \ \ \ (9.7)
$$

The function $\lambda(·)$, used in (9.7), is similar to the one used in (9.6), but with $\delta \equiv 1$. Thus, it is an example of the `varPower(·)` variance function from the $<\mu>$-group (Table 7.4).

Note that, according to (9.7), $\sigma_{it}/\mu_{it} = \sigma$ , i.e., the scale parameter can be interpreted as a coefficient of variation, constant for all timepoints.

## 9.3.1 Fitting the Models Using the gls() Function

we fit the models, M9.2--M9.5, which have the same mean structure as model M9.1, but employ the variance functions from different groups.

In addition, we illustrate model selection using the REML-based LR tests and AIC.

All models are fitted using the generic function update() to modify the weights argument.

```{r "R9.2"}
## (a) Models with various variance functions
fm9.2 <- ## M9.2 from M9.1
  update(fm9.1, 
         ## (9.4), <d>-group
         weights = varPower(form = ~time)) 

fm9.3 <- # M9.3 from M9.1
  update(fm9.1,
         ## (9.5), strata=treat.f
         weights = varPower(form = ~time | treat.f))

fm9.4 <- # M9.4 from M9.1
  update(fm9.1, 
         ## (9.6), <d, m>-group
         weights = varPower())

fm9.5 <- ## M9.5 from M9.1
  update(fm9.1,
         ## (9.7), <\mu>-group
         weights = varPower(fixed = 1)) 


fm9.1$modelStruct$varStruct
fm9.2$modelStruct$varStruct
fm9.3$modelStruct$varStruct
fm9.4$modelStruct$varStruct
fm9.5$modelStruct$varStruct

(intervals(fm9.1, which = "var-cov")) 
(intervals(fm9.2, which = "var-cov")) 
(intervals(fm9.3, which = "var-cov")) 
(intervals(fm9.4, which = "var-cov")) 
(intervals(fm9.5, which = "var-cov")) 

## (b) Test of the variance structure: 
## equal power of time for the two treatments
## M9.2 nested M9.3
anova(fm9.2, fm9.3) 


## (c) Test of the variance structure: 
## power of time vs. timepoint-specific variances
# M9.2 nested M9.1
anova(fm9.2, fm9.1) 


## (d) Test of the variance structure: 
## power of the mean value equal to 1
## M9.5 nested M9.4
anova(fm9.5, fm9.4) 


## (e) AIC for models M9.1– M9.5
## Nonnested models
AIC(fm9.1, fm9.2, 
    fm9.3, fm9.4, fm9.5)
 # Smaller AIC better fit
```

#### 9.3.1.1 Inference and Model Selection

-   model M9.2 is nested both in M9.1 and in M9.3.

-   Using the LR test based on models M9.2 and M9.3 allows testing the hypothesis that the power variance function for the two treatment groups is actually the same.

-   note that the models have the same mean structure and that the test is based on the restricted likelihood, as required when the LR test is applied to verify hypotheses about variance-function parameters

-   a common-power variance function of the TIME covariate can be used for both treatment groups

We may now ask the question whether the common-power variance function can be used as a more parsimonious representation of the variance structure of the data.

-   The null hypothesis for M9.1 and M9.2$\sigma^2_1 = 4^\delta\sigma^2,\ \sigma^2_2 = 12^\delta\sigma^2,\ \sigma^2_3 = 24^\delta\sigma^2,\ and\ \sigma^2_4 = \sigma ^2 52^\delta$

-   It suggests that the fit of model M9.2, measured by the value of the restricted log-likelihood, is not statistically significantly worse than the fit of model M9.1.

-   Hence, model M9.2, which specifies that the variance is a power function of the time (in weeks), offers an adequate description of the variance structure of the data.

-   the REML-based LR test carried out using the likelihoods for models M9.4 and M9.5.

-   The test verifies the null hypothesis, implied by model M9.5, that, if we assume a variance function in the form of a power function of the mean value, the power coefficient is equal to 1.

-   M9.2 seems to offer an adequate description of variance structure of the data.

-   on the other hand, of models M9.4 and M9.5, the former is more appropriate.

-   which of the models M9.2 or M9.4 fits the data better.

-   the models are not nested, so we cannot compare them with the use of the LR test.

-   toward this aim, we need to apply the information criteria

-   thus, based on the information criterion, model M9.2 offers the most adequate description of the data.

-   model M9.3, as it has just been mentioned, is not the best model in a statistical sense, but it nicely illustrates several features related to the use of variance functions like, e.g., the use of a strata for the variance parameters, which are not present in the other considered models.

$$
Var(VISUAL_{it}) = \sigma^2_t \approx (6 \times TIME_{it}^{0.25})^2 = 36 \times \sqrt{TIME_{it}}
$$

-   There are virtually no differences in the estimates of the fixed-effects coefficients for models M9.1--M9.3. In this respect, model M9.5 is the most distinct one.

-   All the models suggest an increasing, negative effect of the "active" treatment compared to placebo.

-   The estimated standard errors of the fixed-effects coefficients vary more noticeably between all the models.

-   This is related to the differences in the assumed residual-variance structure; as it was noted in Sect. 7.8.2, the precision of estimates of $\beta$ depends on the (correct) specification of the structure.

```{r "R9.3"}
## (a) Model M9.2: Power-of-time variance function
mSt2 <- fm9.2$modelStruct # Model structure
vF2 <- mSt2$varStruct # Variance function:(9.4)
summary(vF2) # Summary: delta
summary(fm9.2)$sigma # sigma

## (b) Model M9.3: Power-of-time with treatment-specific coefficients
mSt3 <- fm9.3$modelStruct # Model structure
vF3 <- mSt3$varStruct # Variance function:(9.5)
summary(vF3) # Summary: d1, d2

coef(vF3) # d1, d2
formula(vF3) # Variance function formula
varWeights(vF3)[3:10] # Weights for two subjects

```

### 9.3.2 Model-Fit Evaluation

we know that the model does not offer a proper description of the data, because it ignores the within-subject correlation between the visual acuity measurements.

we will assess the fit of the model using residual plots.

```{r "R9.4", fig.height=5, fig.width=5}
## (a) Raw residuals
library(lattice)
plot(fm9.2, # Fig. 9.1a
     resid(., type = "response") ~ fitted(.)) # Raw vs. fitted
plot(fm9.2, # Raw vs. time (not shown)
     resid(., type = "response") ~ time) # (See Fig. 9.1a)

bwplot(resid(fm9.2) ~ time.f, # Fig. 9.1b
       pch = "|", data = armd) # Raw vs. time.f.


## (b) Pearson residuals
plot(fm9.2, # Fig. 9.1c
     resid(., type = "pearson" ) ~ fitted(.)) # Pearson vs. fitted
plot(fm9.2, # vs. time (not shown)
     resid(., type = "pearson") ~ time) # (See Fig. 9.1c)
bwplot( # Fig. 9.1d
  resid(fm9.2, type = "pearson") ~ time.f, # Pearson vs. time.f
  pch = "|", data = armd)


## (c) Scale-location plots
plot(fm9.2, # Fig. 9.2a
     ## use the scale-location
     sqrt(abs(resid(., type = "response"))) ~ fitted(.),
     type = c("p", "smooth"))
plot(fm9.2, # Fig. 9.2b
     sqrt(abs(resid(., type = "pearson"))) ~ fitted(.),
     type = c("p", "smooth"))
```

-   in Panel R9.4a scatterplots of the residuals versus fitted values and versus the time covariate are created with the help of the plot() function.

-   the first of the plots is shown in Fig. 9.1a.

-   an asymmetric pattern, with large positive (negative) residuals present mainly for small (large) fitted values.

-   we use the function bwplot() from the package lattice (Sect. 3.2.2) to create a box-and-whiskers plot of the residuals for each timepoint.

-   in Panel R9.4b, we create corresponding plots of Pearson residuals (Sect. 7.5.1).

-   the scatterplot of the residuals versus fitted values is shown in Fig. 9.1c.

-   similarly to the plot of the raw residuals, it displays an asymmetric pattern.

-   the box-and-whiskers plots of the Pearson residuals for each timepoint are shown in Fig. 9.1d.

-   the plot for the raw residuals, shown in Fig. 9.2a, suggests a dependence between the residual variance and the mean value.

-   however, this may be an artifact of the heteroscedasticity of the raw residuals, which was observed in Fig. 9.1b.

-   thus, it might be better to look at the scale-location plot for the Pearson residuals.

-   the plot is shown in Fig. 9.2b; it does not indicate any clear trend in the residual variance.

-   Figure 9.3 presents a scatterplot matrix of the Pearson residuals for all four measurement occasions.

-   The figure was constructed using the `splom()` function for the data for 188 subjects with all four post-randomization visual acuity measurements.

-   The 95% confidence ellipses were added using the `ellipse()` function from the ellipse package.

-   The scatterplots clearly show a violation of the assumption of the independence of observations: residuals for different measurement occasions are correlated.

-   The correlation coefficient decreases with the increasing distance between the timepoints.

-   Of course, some caution is needed in interpreting the strength of correlation, because the estimated residuals are correlated even if the independence assumption holds

```{r}
resid9.2 <- resid(fm9.2, type = "response")
presid9.2 <- resid(fm9.2, type = "pearson")
broom.mixed::augment(fm9.2, type.residuals = "pearson")
head(broom.mixed::augment(fm9.2, armd))
## the pearson residuals does not work in the augment function
## need to use the resid() or residuals() function
head(broom.mixed::augment(fm9.2, armd, type.residuals="pearson"))
```
