---
title: "L6.2: Matched Case-Control Studies"
subtitle: "BIOS 6612"
author: "Julia Wrobel"
date: March 1 , 2021
header-includes:
   - \usepackage{bm}
output:
  powerpoint_presentation:
    reference_doc: ../cu_style.pptx
---

```{r, echo= FALSE, include = FALSE}
library(tidyverse)

knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  fig.width = 7,
  fig.height = 5
)

theme_set(theme_bw() + theme(legend.position = "bottom"))
```

## Overview

Today, we cover:

* Matched Case-Control Studies
* Intro to conditional logistic regression
* Review of laws of probability

<br>

Optional readings:

* Agresti: 7.3, 11.2


## Matched case-control studies

*Matching* is one manner in which investigators can control for confounding and bias and also increase precision. Matching consists of pairing or grouping subjects based on pre-specified characteristics. It is a form of stratification that is implemented in the design stage. 

<br>

In a matched case-control study, each person with disease who is included in the study as a case is matched with one or more disease-free people (the controls).

* Matched based on covariate(s) of interest, often age, ethnicity
* Can also match on residential area or workplace to account for confounding variables that are not easily measured

## Matched case-control studies

Design with *M* controls per case is known as a 1:*M* matched study

<br>

* The *matched set* refers to the case and each of the *M* controls matched to that case
* Ususally $1\le M \le 5$ (does not have to be the same for all cases)
* Can also have more than one case

## Example: Herniated Discs and Driving

We are interested in understanding if driving motor vehicles leads to increased risk of lower back pain due to acute herniated lumbar intervertebral discs.

* 1:1 matching was used in this study (Kelsey and Hardy, 1975)
* *Cases* were selected from people aged 20-64 in Connecticut who were diagnosed as having herniated discs and had recently acquired symptoms
* *Controls* were sampled from patients admitted to the same hospital/clinic as a case with a condition not related to the spine
  * Age (age of a case and control were within 10 years) and sex were used to further match
* Total of 217 matched pairs


After an individual was selected for inclusion, data on driving habits and place of residence (the exposures of interest) was obtained for each.


## Modeling data from matched studies

*If you match by design, your analysis must take matching into account!* 

* Probability of disease is assumed to depend on values of explanatory variables.
  * These variables may represent exposures of interest and any confounders not used in the matching process
* Values of matching variables will generally differ between matched sets.
  * Matching variables are *not* included as covariates in the model

<br>

Why can't we just included matching variables as covariates?

* 1980, MC Pike showed that this leads to extreme bias in the OR estimates for the covariates of interest
* $OR_{\text{ignoring matching}} = (OR_{\text{true}})^2$


## Some notation

Let $p_k(X_{ik})$ represent the probability that the *i*th person in the *k*th matched set has disease

* $k \in (1, 2, 3, \ldots K)$ indicates the current matched set
* $i \in (0, 1, 2, \ldots M+1)$ indicates individual in the current matched set
  * $i = 0$ is the case
* $\boldsymbol{X}_{ik}$ is vector of covariates for the *i*th person in the *k*th matched set
  * $\boldsymbol{X}_{ik} = X_{ik1}, X_{ik2}, \ldots, X_{ikp}$
  * These are *NOT* the covariates that were matched on, those are implicitly included in the model via stratification
 
## Modeling data from matched studies 

The model is 

<br>

$$\text{logit} \left\{ p_k(X_{ik}) \right\} = \alpha_k + \beta_1 X_{ik1} + \beta_2X_{ik2} + \ldots + \beta_p x_{ikp}$$

<br>

Note that each matched set/pair has a different intercept, $\alpha_j$

* $\alpha_k$ summarizes the effect of the matching variables on the probability of disease
* $\beta_p$ is the logOR for diseased comparing those exposed to covariate $p$ to those unexposed to covariate $p$

## Constructing likelihood to find beta coefficients

By definition, all individuals in a matched set will have the same values for the matching variables.

* Probability of selection for inclusion in the study will depend on these matching variables
* Selection probability must still be independent of the other explanatory variables in the model

<br>

We need to construct a *conditional likelihood* to take account of the matching in the analysis.

* This is a product of *n* independent terms
* Each of these is the conditional probability that the case in a particular matched set is the one with a particular observed value for the explanatory variables
* Conditional on the values of all the explanatory variables observed in that particular matched set



## Constructing the conditional likelihood

How do we construct a likelihood function that allows us to find estimates for the parameters of interest, $\beta$?

<br>

1. Find the conditional likelihood for the $k$th stratum/matched set
1. Combine likelihoods to obtain likelihood over all strata

<br>

Essentially, likelihood in $k$th stratum = P(observed covariates | number of subjects in $k$th stratum, number of cases in $k$th stratum)

## Conditional logistic regression

The logistic model that we will construct will assume the underlying log odds of disease may differ from pair to pair, but that the log odds ratio comparing the exposed to the unexposed subject will remain the same across all matched pairs.

<br>

* This means that the underlying odds of disease can be different from one pair (or set) to another, but the effect of the (non-matched) predictor is the same in all matched pairs



## Laws of Probability

A refresher on laws of probability that we'll need to derive the likelihood for conditional logistic regression. Take events $A$ and $B$.

<br>

* $P(A|B)$ is the *conditional probability* that event A occurs given that event B occurs
* $P (A \cap B)$ is the *joint probability* that events A and B occur together

<br>

$$P(A|B) = \frac{P (A \cap B)}{P(B)}$$

* If events A and B are independent, then their joint probability is 

$$P (A \cap B) = P(A)P(B)$$

## Law of Total Probability

Let $B_1, B_2, \ldots, B_n$ be a set of mutually exclusive events such that $\sum_j ^n P(B_j) = 1$. The law of total probability states that:

<br>

$$
\begin{split}
P(A) &= P(A|B_1)P(B_1) + P(A|B_2)P(B_2) + \ldots P(A|B_n)P(B_n)\\[5mm]
&= \sum_j^n P(A|B_j)P(B_j)
\end{split}
$$

## Bayes' Theorem

Bayes' Theorem states that

$$P(B_j | A) = \frac{P(A|B_j)P(B_j)}{P(A)}; i = 1,\ldots,n$$


<br>

Using the law of total probability, we can substitue $P(A)$ in the denominator to get

$$P(B_j | A) = \frac{P(A|B_j)P(B_j)}{ \sum_j^n P(A|B_j)P(B_j)}$$
